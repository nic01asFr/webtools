"""
Intelligent Orchestrator - Orchestrateur Intelligent Multi-Outils

SystÃ¨me qui:
1. ConnaÃ®t tous les outils disponibles et leurs capacitÃ©s
2. Analyse la requÃªte et le contexte
3. Planifie la meilleure stratÃ©gie
4. ExÃ©cute en s'adaptant aux rÃ©sultats
5. Traite et enrichit les donnÃ©es
6. Ajuste et poursuit jusqu'Ã  obtenir la rÃ©ponse

Outils disponibles:
- SearXNG: Recherche web pour dÃ©couvrir sources
- ExtractAgent: Extraction de contenu web
- AdaptiveNavigator: API/Sites avec stratÃ©gies adaptatives
- SearchSite: Recherche interactive sur sites
- DataProcessor: Traitement datasets
- Vision: Analyse d'images
"""

import logging
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
from enum import Enum

from app.agents.adaptive_navigator import AdaptiveNavigator, StrategyType
from app.agents.data_processor import DataProcessor, DataProcessorFactory
from app.core.llm.base import BaseLLMClient
from app.services.search_service import searxng_client
from app.manager import ExtractorManager
from app.api.models import ExtractionOptions
from app.core.data_extractor import GenericDataExtractor, DataValidator

logger = logging.getLogger(__name__)


class ToolType(str, Enum):
    """Types d'outils disponibles."""
    SEARCH_WEB = "search_web"          # SearXNG pour dÃ©couvrir sources
    EXTRACT_CONTENT = "extract_content"  # Extraction contenu web
    API_NAVIGATE = "api_navigate"      # Navigation API
    SEARCH_SITE = "search_site"        # Recherche sur site
    PROCESS_DATA = "process_data"      # Traitement donnÃ©es
    ANALYZE_IMAGE = "analyze_image"    # Analyse d'images


class Tool:
    """ReprÃ©sentation d'un outil avec ses capacitÃ©s."""

    def __init__(
        self,
        name: str,
        tool_type: ToolType,
        capabilities: List[str],
        input_formats: List[str],
        output_format: str,
        best_for: List[str],
        limitations: List[str]
    ):
        self.name = name
        self.tool_type = tool_type
        self.capabilities = capabilities
        self.input_formats = input_formats
        self.output_format = output_format
        self.best_for = best_for
        self.limitations = limitations


class ExecutionContext:
    """Contexte d'exÃ©cution avec historique et apprentissage."""

    def __init__(self, query: str):
        self.query = query
        self.steps: List[Dict] = []
        self.datasets: Dict[str, List[Dict]] = {}
        self.discovered_sources: List[str] = []
        self.tool_success_rate: Dict[str, Dict] = {}

        # NOUVEAU: Contenu final structurÃ© avec accumulation par section
        self.final_content: Dict[str, Any] = {
            "sections": {},  # {section_name: {"raw_data": [], "content": "", "metadata": {}}}
            "global_metadata": {
                "sources_used": [],
                "extraction_timestamps": [],
                "all_structured_data": []
            }
        }

    def initialize_sections(self, section_names: List[str]):
        """Initialise les sections du rapport."""
        for section_name in section_names:
            self.final_content["sections"][section_name] = {
                "raw_data": [],
                "content": "",
                "metadata": {"data_count": 0, "sources": []}
            }
        logger.info(f"ðŸ“‹ Sections initialisÃ©es: {', '.join(section_names)}")

    def add_step(self, tool: str, action: str, input_data: Any, output_data: Any, success: bool):
        """Enregistre une Ã©tape."""
        input_str = str(input_data)[:100]
        output_str = str(output_data)[:100] if output_data else ""

        self.steps.append({
            "step_number": len(self.steps) + 1,
            "tool": tool,
            "action": action,
            "success": success,
            "timestamp": datetime.now().isoformat(),
            "input_preview": input_str,
            "output_preview": output_str
        })

        # Mettre Ã  jour taux de succÃ¨s
        if tool not in self.tool_success_rate:
            self.tool_success_rate[tool] = {"success": 0, "failures": 0}

        if success:
            self.tool_success_rate[tool]["success"] += 1
        else:
            self.tool_success_rate[tool]["failures"] += 1

    def add_dataset(self, name: str, data: List[Dict]):
        """Ajoute un dataset."""
        self.datasets[name] = data
        logger.info(f"ðŸ’¾ Dataset '{name}' ajoutÃ©: {len(data)} items")

    def get_all_data(self) -> List[Dict]:
        """RÃ©cupÃ¨re toutes les donnÃ©es."""
        all_data = []
        for dataset in self.datasets.values():
            all_data.extend(dataset)
        return all_data


class IntelligentOrchestrator:
    """
    Orchestrateur intelligent qui connaÃ®t ses outils et s'adapte dynamiquement.
    """

    def __init__(self, llm_client: BaseLLMClient, timeout: int = 300):
        self.llm_client = llm_client
        self.timeout = timeout

        # Initialiser les outils
        self.adaptive_navigator = AdaptiveNavigator(llm_client, timeout)
        self.extractor_manager = ExtractorManager()
        self.data_extractor = GenericDataExtractor(llm_client)

        # Catalogue d'outils
        self.tools = self._init_tools_catalog()

    def _init_tools_catalog(self) -> Dict[str, Tool]:
        """Initialise le catalogue d'outils."""
        return {
            "searxng": Tool(
                name="SearXNG",
                tool_type=ToolType.SEARCH_WEB,
                capabilities=["dÃ©couvrir_sources", "recherche_web", "trouver_urls"],
                input_formats=["query_text"],
                output_format="list_of_urls",
                best_for=["exploration", "dÃ©couverte", "sources_multiples"],
                limitations=["pas_de_contenu_direct", "qualitÃ©_variable"]
            ),
            "extract": Tool(
                name="WebExtractor",
                tool_type=ToolType.EXTRACT_CONTENT,
                capabilities=["extraire_contenu", "suivre_liens", "parser_html"],
                input_formats=["url"],
                output_format="structured_content",
                best_for=["contenu_pages", "articles", "documentation"],
                limitations=["sites_complexes", "javascript_lourd"]
            ),
            "api_navigator": Tool(
                name="API Navigator",
                tool_type=ToolType.API_NAVIGATE,
                capabilities=["appeler_apis", "parser_docs", "construire_requÃªtes"],
                input_formats=["api_url", "query"],
                output_format="structured_data",
                best_for=["apis_rest", "donnÃ©es_structurÃ©es", "classements"],
                limitations=["nÃ©cessite_doc", "apis_complexes"]
            ),
            "search_site": Tool(
                name="SearchSite",
                tool_type=ToolType.SEARCH_SITE,
                capabilities=["recherche_interactive", "formulaires", "navigation"],
                input_formats=["site_url", "query"],
                output_format="search_results",
                best_for=["sites_avec_recherche", "exploration_site"],
                limitations=["sites_sans_formulaire", "lent"]
            ),
            "data_processor": Tool(
                name="DataProcessor",
                tool_type=ToolType.PROCESS_DATA,
                capabilities=["filtrer", "trier", "grouper", "joindre", "transformer"],
                input_formats=["list_of_dicts"],
                output_format="processed_data",
                best_for=["post_traitement", "agrÃ©gations", "nettoyage"],
                limitations=["nÃ©cessite_donnÃ©es_structurÃ©es"]
            )
        }

    async def execute_intelligent_research(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None,
        max_steps: int = 10
    ) -> Dict[str, Any]:
        """
        Recherche intelligente adaptative - ARCHITECTURE REFONDÃ‰E.

        Nouveau processus en 4 phases (vision utilisateur):
        1. PHASE 1: EXPLORATION + PLAN DÃ‰TAILLÃ‰
           - Recherches exploratoires ciblÃ©es et restreintes
           - Ã‰valuation du champ disponible
           - Plan avec structure complÃ¨te, enchaÃ®nements, canvas dÃ©taillÃ©

        2. PHASE 2: CONSTRUCTION ITÃ‰RATIVE SECTION PAR SECTION
           - Pour chaque section: recherches, extraction, analyse, croisement
           - Canvas rempli progressivement

        3. PHASE 3: COHÃ‰RENCE GLOBALE PARTIE PAR PARTIE
           - Revue inter-sections
           - CohÃ©rence narrative

        4. PHASE 4: FINALISATION
           - Bibliographie et traces complÃ¨tes
        """
        logger.info(f"ðŸ§  Recherche intelligente REFONDÃ‰E: {query}")
        start_time = datetime.now()

        ctx = ExecutionContext(query)
        context = context or {}
        self.current_context = context

        # ===================================================================
        # PHASE 1: EXPLORATION + PLAN DÃ‰TAILLÃ‰ avec canvas complet
        # ===================================================================
        logger.info("ðŸ” PHASE 1: Exploration + Planification dÃ©taillÃ©e")

        # Ã‰tape 1.1: Exploration initiale restreinte pour Ã©valuer le champ
        exploration_data = await self._exploratory_phase(query, context)
        logger.info(f"  âœ“ Exploration: {len(exploration_data.get('sources', []))} sources dÃ©couvertes")
        logger.info(f"  âœ“ Champ Ã©valuÃ©: {exploration_data.get('field_assessment', 'N/A')}")

        # Ã‰tape 1.2: Planification avec canvas dÃ©taillÃ© basÃ© sur exploration
        plan = await self._create_detailed_plan(query, context, exploration_data)
        logger.info(f"  âœ“ Canvas structure: {len(plan.get('sections', []))} sections")
        logger.info(f"  âœ“ Profondeur: {plan.get('complexity_analysis', {}).get('target_length', 'N/A')}")
        logger.info(f"  âœ“ EnchaÃ®nements dÃ©finis: {len(plan.get('narrative_flow', []))} transitions")

        # Initialiser les sections du canvas
        if 'sections' in plan and plan['sections']:
            ctx.initialize_sections(plan['sections'])
            logger.info(f"  âœ“ Canvas initialisÃ©: {len(plan['sections'])} sections")

        # ===================================================================
        # PHASE 2: CONSTRUCTION ITÃ‰RATIVE SECTION PAR SECTION
        # ===================================================================
        logger.info("ðŸ—ï¸  PHASE 2: Construction itÃ©rative section par section")

        # Pour chaque section du canvas, processus complet
        for section_name, section_config in plan.get('section_targets', {}).items():
            logger.info(f"\n  ðŸ“ Section: {section_name}")
            logger.info(f"     Profondeur: {section_config.get('depth', 'moderate')}")
            logger.info(f"     Objectif: ~{section_config.get('words_target', 500)} mots")

            # Ã‰tape 2.1: Recherches ciblÃ©es pour cette section
            section_data = await self._section_research_phase(
                query=query,
                section_name=section_name,
                section_config=section_config,
                exploration_data=exploration_data,
                context=ctx
            )
            logger.info(f"     âœ“ {len(section_data.get('sources', []))} sources collectÃ©es")

            # Ã‰tape 2.2: Extraction et analyse des sources
            extracted_data = await self._section_extraction_phase(
                section_name=section_name,
                section_data=section_data,
                context=ctx
            )
            logger.info(f"     âœ“ {len(extracted_data)} contenus extraits")

            # Ã‰tape 2.3: Croisement avec donnÃ©es existantes
            enriched_data = self._cross_reference_data(
                section_name=section_name,
                new_data=extracted_data,
                context=ctx
            )
            logger.info(f"     âœ“ {len(enriched_data)} donnÃ©es enrichies")

            # Ã‰tape 2.4: SynthÃ¨se de la section
            section_content = await self._synthesize_single_section(
                query=query,
                section_name=section_name,
                section_config=section_config,
                enriched_data=enriched_data,
                context=ctx
            )
            logger.info(f"     âœ“ Contenu gÃ©nÃ©rÃ©: {len(section_content.split())} mots")

            # Stocker dans le canvas
            ctx.final_content['sections'][section_name]['content'] = section_content
            ctx.final_content['sections'][section_name]['raw_data'] = enriched_data

        # ===================================================================
        # PHASE 3: COHÃ‰RENCE GLOBALE PARTIE PAR PARTIE
        # ===================================================================
        logger.info("\nðŸ”— PHASE 3: Revue de cohÃ©rence globale")

        # Ã‰tape 3.1: Analyse inter-sections
        coherence_analysis = await self._analyze_intersections(
            query=query,
            plan=plan,
            context=ctx
        )
        logger.info(f"  âœ“ {len(coherence_analysis.get('improvements', []))} amÃ©liorations identifiÃ©es")

        # Ã‰tape 3.2: Application des amÃ©liorations de cohÃ©rence
        await self._apply_coherence_improvements(
            coherence_analysis=coherence_analysis,
            context=ctx
        )
        logger.info(f"  âœ“ CohÃ©rence narrative amÃ©liorÃ©e")

        # ===================================================================
        # PHASE 4: FINALISATION
        # ===================================================================
        logger.info("\nâœ… PHASE 4: Finalisation")

        # Ã‰tape 4.1: Assemblage final
        final_answer = await self._final_assembly(query, plan, ctx)

        # Ã‰tape 4.2: GÃ©nÃ©ration bibliographie complÃ¨te
        logger.info("  âœ“ Bibliographie gÃ©nÃ©rÃ©e")

        # Ã‰tape 4.3: GÃ©nÃ©ration des traces complÃ¨tes
        final_answer = self._add_complete_traces(final_answer, ctx, exploration_data, plan)

        processing_time = (datetime.now() - start_time).total_seconds()

        return {
            "success": True,
            "query": query,
            "answer": final_answer,
            "execution_context": {
                "steps_executed": len(ctx.steps),
                "datasets_collected": len(ctx.datasets),
                "sources_discovered": ctx.discovered_sources,  # Liste complÃ¨te, pas len()
                "tool_performance": ctx.tool_success_rate
            },
            "processing_time": processing_time
        }

    async def _analyze_and_plan(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        Analyse la requÃªte et planifie la stratÃ©gie avec les outils disponibles.
        """
        # Construire description des outils
        tools_desc = []
        for tool in self.tools.values():
            tools_desc.append({
                "name": tool.name,
                "type": tool.tool_type,
                "best_for": tool.best_for,
                "output": tool.output_format
            })

        # DÃ©tecter si on a une URL ou source prÃ©cise
        has_url = bool(context.get("url") or context.get("sources_required"))

        # Extraire les sections demandÃ©es (si prÃ©sentes dans le contexte)
        requested_sections = context.get("output_sections") or []

        prompt = f"""Analyse cette requÃªte et planifie la stratÃ©gie ADAPTÃ‰E avec les outils disponibles:

REQUÃŠTE: "{query}"

CONTEXTE:
{json.dumps(context, indent=2) if context else "Aucun"}
URL ou source spÃ©cifique fournie: {"OUI" if has_url else "NON"}
Sections demandÃ©es: {requested_sections if requested_sections else "Ã€ dÃ©terminer"}

OUTILS DISPONIBLES:
{json.dumps(tools_desc, indent=2)}

Ta mission: CrÃ©er un plan d'exÃ©cution optimal ADAPTÃ‰ Ã  la complexitÃ© et profondeur attendue.

Ã‰TAPE 1: ANALYSE DE LA REQUÃŠTE
Ã‰value ces critÃ¨res pour dÃ©terminer la profondeur nÃ©cessaire:

1. **ComplexitÃ© du sujet** (1-5):
   - 1 = Simple, concept unique (ex: "c'est quoi X?")
   - 3 = ModÃ©rÃ©, plusieurs aspects (ex: "avantages et inconvÃ©nients de X")
   - 5 = Complexe, multidimensionnel (ex: "Ã©cosystÃ¨me complet, tendances, adoption, futur de X")

2. **SpÃ©cificitÃ© demandÃ©e** (1-5):
   - 1 = TrÃ¨s large, vue gÃ©nÃ©rale
   - 3 = FocalisÃ© sur certains aspects
   - 5 = TrÃ¨s prÃ©cis, dÃ©tails techniques

3. **Format demandÃ©** (1-5):
   - 1 = RÃ©sumÃ© court, dÃ©finition
   - 3 = Rapport standard
   - 5 = Ã‰tude approfondie, analyse dÃ©taillÃ©e

4. **Profondeur temporelle** (1-5):
   - 1 = Point dans le temps (ex: "aujourd'hui")
   - 3 = PÃ©riode dÃ©finie (ex: "2024")
   - 5 = Ã‰volution historique + projection future

5. **Interconnexions attendues** (1-5):
   - 1 = Sujet isolÃ©
   - 3 = Relations avec contexte
   - 5 = Analyse systÃ©mique, impacts multiples

Ã‰TAPE 2: DÃ‰TERMINER L'AMPLEUR

Calcule score_profondeur = moyenne des 5 critÃ¨res

- Score 1.0-2.0 â†’ **Rapport CONCIS** (1-2 sections, 500-1000 mots total)
- Score 2.1-3.0 â†’ **Rapport STANDARD** (2-3 sections, 1000-1500 mots total)
- Score 3.1-4.0 â†’ **Rapport DÃ‰TAILLÃ‰** (3-5 sections, 1500-2500 mots total)
- Score 4.1-5.0 â†’ **Ã‰tude APPROFONDIE** (4-7 sections, 2500-4000 mots total)

Retourne JSON:
{{
  "strategy_type": "direct|exploration|hybrid",
  "reasoning": "Explication de la stratÃ©gie",
  "complexity_analysis": {{
    "topic_complexity": 1-5,
    "specificity": 1-5,
    "format_depth": 1-5,
    "temporal_depth": 1-5,
    "interconnections": 1-5,
    "overall_score": moyenne,
    "target_length": "concis|standard|dÃ©taillÃ©|approfondi",
    "estimated_words": nombre_mots_total,
    "justification": "pourquoi ce niveau de dÃ©tail"
  }},
  "data_needed": ["liste des donnÃ©es nÃ©cessaires"],
  "sections": ["liste des sections du rapport"] ou null si pas de rapport,
  "section_targets": {{
    "nom_section": {{"words_target": nombre_mots, "depth": "light|moderate|deep"}}
  }},
  "steps": [
    {{
      "step": 1,
      "tool": "nom_outil",
      "action": "description de l'action",
      "input": {{"query": "...", "max_results": 10}},
      "target_section": "nom de la section cible" ou null,
      "expected_output": "ce qu'on attend"
    }}
  ],
  "expected_iterations": 1-5,
  "data_processing_needed": true|false
}}

STRATÃ‰GIES OBLIGATOIRES:
- **Si URL/source fournie**: utiliser api_navigator ou search_site (stratÃ©gie "direct")
- **Si AUCUNE URL/source**: TOUJOURS commencer par "searxng" pour dÃ©couvrir des sources (stratÃ©gie "exploration")
- **hybrid**: Combiner plusieurs outils

RÃˆGLES CRITIQUES:
1. Sans URL spÃ©cifique, la premiÃ¨re Ã©tape DOIT Ãªtre "searxng"
2. AprÃ¨s searxng, prÃ©voir "webextractor" pour extraire le contenu des URLs trouvÃ©es
3. Les Ã©tapes doivent Ãªtre enchaÃ®nÃ©es logiquement
4. Maximum 5 Ã©tapes dans le plan initial
5. ADAPTER le nombre de sources Ã  collecter selon la profondeur (score 1-2 = 5 sources, score 4-5 = 15 sources)

EXEMPLE pour requÃªte SIMPLE "c'est quoi Rust":
{{
  "strategy_type": "exploration",
  "complexity_analysis": {{
    "topic_complexity": 1,
    "specificity": 1,
    "format_depth": 1,
    "temporal_depth": 1,
    "interconnections": 1,
    "overall_score": 1.0,
    "target_length": "concis",
    "estimated_words": 600,
    "justification": "RequÃªte simple demandant dÃ©finition basique"
  }},
  "sections": ["DÃ©finition", "Principaux usages"],
  "section_targets": {{
    "DÃ©finition": {{"words_target": 300, "depth": "light"}},
    "Principaux usages": {{"words_target": 300, "depth": "light"}}
  }},
  "steps": [
    {{"step": 1, "tool": "searxng", "action": "Rechercher sources", "input": {{"query": "...", "max_results": 5}}}},
    {{"step": 2, "tool": "webextractor", "action": "Extraire contenu", "input": {{"urls": []}}}}
  ]
}}

EXEMPLE pour requÃªte APPROFONDIE "Ã©cosystÃ¨me Rust 2024, adoption entreprise, roadmap":
{{
  "strategy_type": "exploration",
  "complexity_analysis": {{
    "topic_complexity": 5,
    "specificity": 4,
    "format_depth": 5,
    "temporal_depth": 4,
    "interconnections": 5,
    "overall_score": 4.6,
    "target_length": "approfondi",
    "estimated_words": 3500,
    "justification": "RequÃªte complexe nÃ©cessitant analyse multidimensionnelle avec donnÃ©es 2024, tendances adoption, et projection future"
  }},
  "sections": ["Vue d'ensemble", "Ã‰cosystÃ¨me technique", "Adoption entreprise", "Cas d'usage", "Roadmap et futur", "DÃ©fis et perspectives"],
  "section_targets": {{
    "Vue d'ensemble": {{"words_target": 400, "depth": "moderate"}},
    "Ã‰cosystÃ¨me technique": {{"words_target": 700, "depth": "deep"}},
    "Adoption entreprise": {{"words_target": 700, "depth": "deep"}},
    "Cas d'usage": {{"words_target": 600, "depth": "moderate"}},
    "Roadmap et futur": {{"words_target": 600, "depth": "deep"}},
    "DÃ©fis et perspectives": {{"words_target": 500, "depth": "moderate"}}
  }},
  "steps": [
    {{"step": 1, "tool": "searxng", "action": "Rechercher sources", "input": {{"query": "...", "max_results": 15}}}},
    {{"step": 2, "tool": "webextractor", "action": "Extraire contenu", "input": {{"urls": []}}}}
  ]
}}
"""

        response = await self.llm_client.generate(
            [{"role": "user", "content": prompt}],
            max_tokens=2000,
            temperature=0.2
        )

        # Parser JSON
        try:
            start_idx = response.find('{')
            if start_idx != -1:
                bracket_depth = 0
                for i, char in enumerate(response[start_idx:], start=start_idx):
                    if char == '{':
                        bracket_depth += 1
                    elif char == '}':
                        bracket_depth -= 1
                        if bracket_depth == 0:
                            plan = json.loads(response[start_idx:i + 1])
                            return plan
        except:
            pass

        # Fallback: plan exploration avec SearXNG
        return {
            "strategy_type": "exploration",
            "reasoning": "Plan par dÃ©faut - exploration web",
            "data_needed": ["sources web"],
            "sections": requested_sections or ["SynthÃ¨se"],  # Sections par dÃ©faut
            "steps": [
                {
                    "step": 1,
                    "tool": "searxng",
                    "action": "Rechercher des sources sur le web",
                    "input": {"query": query, "max_results": 10},
                    "target_section": requested_sections[0] if requested_sections else "SynthÃ¨se",
                    "expected_output": "liste d'URLs"
                },
                {
                    "step": 2,
                    "tool": "webextractor",
                    "action": "Extraire le contenu des pages trouvÃ©es",
                    "input": {"urls": []},
                    "target_section": requested_sections[0] if requested_sections else "SynthÃ¨se",
                    "expected_output": "contenu extrait"
                }
            ],
            "expected_iterations": 2,
            "data_processing_needed": False
        }

    async def _execute_step(self, step: Dict, ctx: ExecutionContext) -> Dict[str, Any]:
        """ExÃ©cute une Ã©tape avec l'outil appropriÃ©."""
        tool_name = step.get("tool", "").lower().replace(" ", "_").replace("-", "_")
        action = step.get("action", "")
        input_data = step.get("input", {})

        logger.info(f"  ðŸ”§ Outil: {tool_name}")

        # Normaliser les noms d'outils
        if tool_name in ["searchsite", "site_search"]:
            tool_name = "search_site"
        elif tool_name in ["dataprocessor", "processor"]:
            tool_name = "data_processor"

        try:
            # Router vers le bon outil
            if tool_name in ["api_navigator", "api", "navigator"]:
                # RÃ©cupÃ©rer l'URL - soit de l'input, soit du step, soit du contexte initial
                target_url = (input_data.get("url") or
                            step.get("url", "") or
                            self.current_context.get("url", ""))

                # Si toujours pas d'URL, essayer de dÃ©tecter depuis la query
                if not target_url and "geo.api.gouv.fr" in ctx.query.lower():
                    target_url = "https://geo.api.gouv.fr"

                logger.info(f"  ðŸŒ URL cible: {target_url}")

                result = await self.adaptive_navigator.execute(
                    user_query=input_data.get("query", ctx.query),
                    target_url=target_url,
                    context=input_data.get("context", {"is_api": True} if "api" in target_url.lower() else {})
                )

                if result.get("success") and result.get("results"):
                    ctx.add_dataset(f"step_{len(ctx.steps)}_data", result["results"])
                    ctx.add_step(tool_name, action, input_data, result["results"], True)
                    return result
                else:
                    ctx.add_step(tool_name, action, input_data, None, False)
                    return {"success": False, "error": "Pas de rÃ©sultats"}

            elif tool_name in ["searxng", "search_web", "web_search"]:
                # Recherche web avec SearXNG
                query = input_data.get("query", ctx.query)
                max_results = input_data.get("max_results", 10)

                logger.info(f"  ðŸ” Recherche SearXNG: {query}")

                results = await searxng_client.search(query, max_results=max_results)

                if results:
                    # Extraire URLs et titres
                    urls_data = []
                    for result in results[:max_results]:
                        urls_data.append({
                            "url": result.url,
                            "title": result.title,
                            "snippet": result.content
                        })
                        # DÃ©duplication des sources
                        if result.url not in ctx.discovered_sources:
                            ctx.discovered_sources.append(result.url)

                    ctx.add_dataset(f"searxng_{len(ctx.steps)}", urls_data)
                    ctx.add_step(tool_name, action, input_data, urls_data, True)

                    logger.info(f"  âœ… {len(urls_data)} URLs trouvÃ©es")
                    return {"success": True, "data": urls_data, "urls": [d["url"] for d in urls_data]}
                else:
                    ctx.add_step(tool_name, action, input_data, None, False)
                    return {"success": False, "error": "Aucun rÃ©sultat"}

            elif tool_name in ["webextractor", "extract", "extract_content"]:
                # Extraction de contenu web
                urls = input_data.get("urls", [])

                # Si pas d'URLs fournies, chercher dans les donnÃ©es prÃ©cÃ©dentes
                if not urls:
                    all_data = ctx.get_all_data()
                    for item in all_data:
                        if isinstance(item, dict) and "url" in item:
                            urls.append(item["url"])

                if not urls:
                    return {"success": False, "error": "Aucune URL Ã  extraire"}

                # Limiter Ã  5 URLs pour ne pas prendre trop de temps
                urls = urls[:5]
                logger.info(f"  ðŸ“„ Extraction: {len(urls)} URLs")

                extracted_data = []
                for url in urls:
                    try:
                        extract_result = await self.extractor_manager.extract(
                            url=url,
                            llm_client=self.llm_client,
                            options=ExtractionOptions(
                                timeout=30,
                                use_agent=False,  # Pas d'agent pour aller plus vite
                                headless=True
                            )
                        )

                        if extract_result.success:
                            # Extraction donnÃ©es structurÃ©es AUTOMATIQUE
                            topic_context = ctx.query if hasattr(ctx, 'query') else ""
                            structured = await self.data_extractor.extract_structured_data(
                                content=extract_result.content or "",
                                source_url=url,
                                topic_context=topic_context
                            )

                            extracted_data.append({
                                "url": url,
                                "title": extract_result.title or "",
                                "content": extract_result.content or "",
                                "content_type": extract_result.content_type or "unknown",
                                "metadata": extract_result.metadata if hasattr(extract_result, 'metadata') else {},
                                "structured_data": structured.to_dict()  # DONNÃ‰ES STRUCTURÃ‰ES
                            })
                            logger.info(f"  âœ… Extraction rÃ©ussie: {url[:60]}... ({len(structured.numerical)} num)")
                        else:
                            logger.warning(f"  âš ï¸ Ã‰chec extraction {url}: {extract_result.error if hasattr(extract_result, 'error') else 'Unknown'}")
                    except Exception as e:
                        logger.error(f"  âŒ Erreur extraction {url}: {e}")
                        continue

                if extracted_data:
                    ctx.add_dataset(f"extracted_{len(ctx.steps)}", extracted_data)
                    ctx.add_step(tool_name, action, input_data, extracted_data, True)
                    logger.info(f"  âœ… {len(extracted_data)} pages extraites")
                    return {"success": True, "data": extracted_data}
                else:
                    ctx.add_step(tool_name, action, input_data, None, False)
                    return {"success": False, "error": "Aucune extraction rÃ©ussie"}

            elif tool_name == "data_processor":
                # Traiter les donnÃ©es existantes
                all_data = ctx.get_all_data()
                if not all_data:
                    return {"success": False, "error": "Pas de donnÃ©es Ã  traiter"}

                operation = input_data.get("operation", "")
                params = input_data.get("params", {})

                processor = DataProcessor(all_data)

                # Appliquer l'opÃ©ration
                if operation == "filter_and_sort":
                    result_data = (processor
                                  .filter_by_field(**params.get("filter", {}))
                                  .sort(**params.get("sort", {}))
                                  .get())
                elif operation == "top_n":
                    result_data = DataProcessorFactory.top_n_by_field(
                        all_data,
                        params.get("field", "value"),
                        params.get("n", 10)
                    )
                elif operation == "aggregate":
                    result_data = DataProcessorFactory.aggregate_by_group(
                        all_data,
                        **params
                    )
                else:
                    result_data = all_data

                ctx.add_dataset(f"processed_{len(ctx.steps)}", result_data)
                ctx.add_step(tool_name, action, input_data, result_data, True)

                return {"success": True, "data": result_data}

            else:
                logger.warning(f"  âš ï¸ Outil inconnu: {tool_name}")
                return {"success": False, "error": f"Outil inconnu: {tool_name}"}

        except Exception as e:
            logger.error(f"Erreur exÃ©cution Ã©tape: {e}", exc_info=True)
            ctx.add_step(tool_name, action, input_data, None, False)
            return {"success": False, "error": str(e)}

    async def _evaluate_result(
        self,
        query: str,
        step_result: Dict,
        ctx: ExecutionContext
    ) -> Dict[str, Any]:
        """Ã‰value le rÃ©sultat et dÃ©termine les prochaines actions."""

        all_data = ctx.get_all_data()

        prompt = f"""Ã‰value le rÃ©sultat de cette Ã©tape de recherche:

REQUÃŠTE INITIALE: "{query}"

DONNÃ‰ES COLLECTÃ‰ES: {len(all_data)} items
Ã‰chantillon: {json.dumps(all_data[:3], indent=2, ensure_ascii=False) if all_data else "Aucune"}

Ã‰TAPES DÃ‰JÃ€ EXÃ‰CUTÃ‰ES: {len(ctx.steps)}

Ã‰value:
{{
  "completeness": 0-100,
  "what_we_have": ["liste"],
  "what_missing": ["liste"],
  "data_quality": "excellent|good|partial|poor",
  "next_actions": [
    {{
      "tool": "nom_outil",
      "action": "description",
      "input": {{}},
      "priority": "high|medium|low",
      "reasoning": "pourquoi"
    }}
  ]
}}

RÃˆGLES:
- completeness: % de rÃ©ponse possible avec donnÃ©es actuelles
- Si >= 90%: next_actions peut Ãªtre vide
- Si < 50%: proposer 2-3 actions
- PrivilÃ©gier traitement donnÃ©es existantes avant nouvelles recherches
"""

        response = await self.llm_client.generate(
            [{"role": "user", "content": prompt}],
            max_tokens=1500,
            temperature=0.2
        )

        # Parser JSON
        try:
            start_idx = response.find('{')
            if start_idx != -1:
                bracket_depth = 0
                for i, char in enumerate(response[start_idx:], start=start_idx):
                    if char == '{':
                        bracket_depth += 1
                    elif char == '}':
                        bracket_depth -= 1
                        if bracket_depth == 0:
                            return json.loads(response[start_idx:i + 1])
        except:
            pass

        # Fallback
        return {
            "completeness": 50 if all_data else 0,
            "what_we_have": [f"{len(all_data)} items"],
            "what_missing": [],
            "next_actions": [],
            "data_quality": "partial"
        }

    async def _synthesize_answer(self, query: str, ctx: ExecutionContext) -> Dict[str, Any]:
        """SynthÃ©tise la rÃ©ponse finale."""
        all_data = ctx.get_all_data()

        if not all_data:
            return {
                "type": "no_data",
                "message": "Aucune donnÃ©e collectÃ©e",
                "steps_attempted": len(ctx.steps)
            }

        # PrÃ©parer les donnÃ©es pour la synthÃ¨se
        # NOUVEAU: SÃ©lection intelligente au lieu de truncation arbitraire
        logger.info(f"  ðŸŽ¯ PrÃ©paration donnÃ©es: {len(all_data)} items disponibles")

        # SÃ©lectionner les chunks les plus pertinents (max 50000 chars)
        selected_data = self._semantic_chunk_selection(all_data, query, max_chars=50000)

        # Extraire le contenu textuel ET donnÃ©es structurÃ©es
        extracted_contents = []
        urls_info = []
        all_structured_data = []

        for item in selected_data:
            if isinstance(item, dict):
                if "content" in item and item.get("content"):
                    # Page web extraite avec contenu (PLUS de truncation ici, dÃ©jÃ  fait par sÃ©lection)
                    page_data = {
                        "url": item.get("url", ""),
                        "title": item.get("title", ""),
                        "content": item.get("content", "")  # Contenu complet du chunk sÃ©lectionnÃ©
                    }

                    # Ajouter donnÃ©es structurÃ©es si disponibles
                    if "structured_data" in item and item["structured_data"]:
                        struct = item["structured_data"]
                        page_data["numerical_data"] = struct.get("numerical", [])
                        page_data["temporal_data"] = struct.get("temporal", [])
                        page_data["entities"] = struct.get("entities", [])
                        all_structured_data.append(struct)

                    extracted_contents.append(page_data)
                elif "url" in item:
                    # URL dÃ©couverte sans contenu
                    urls_info.append({
                        "url": item.get("url", ""),
                        "title": item.get("title", ""),
                        "snippet": item.get("snippet", "")
                    })

        # Utiliser le LLM pour synthÃ©tiser
        data_for_synthesis = {
            "pages_extraites": extracted_contents,
            "urls_trouvÃ©es": urls_info[:5] if len(urls_info) > 5 else urls_info  # Limiter les URLs
        }

        # Validation croisÃ©e des donnÃ©es numÃ©riques
        validated_data = None
        if all_structured_data:
            from app.core.data_extractor import StructuredData
            structured_objects = []
            for sd in all_structured_data:
                # Reconstruire objets StructuredData
                try:
                    structured_objects.append(StructuredData(
                        source_url=sd.get("source_url", ""),
                        numerical=[],  # SimplifiÃ© pour validation
                        temporal=[],
                        entities=[],
                        relationships=[],
                        extraction_timestamp=sd.get("extraction_timestamp", ""),
                        overall_confidence=sd.get("overall_confidence", 0.0)
                    ))
                except:
                    pass

            if structured_objects:
                common_metrics = DataValidator.find_common_metrics(structured_objects)
                if common_metrics:
                    validated_data = DataValidator.validate_numerical_coherence(common_metrics)
                    logger.info(f"âœ“ Validation croisÃ©e: {len(common_metrics)} mÃ©triques communes trouvÃ©es")

        # RÃ©cupÃ©rer le format de sortie demandÃ©
        output_structure = self.current_context.get("output_structure", "summary")
        output_sections = self.current_context.get("output_sections", [])

        # Si un rapport structurÃ© est demandÃ©, gÃ©nÃ©rer les sections
        if output_structure == "report" and output_sections:
            # PrÃ©parer rÃ©sumÃ© des donnÃ©es structurÃ©es validÃ©es
            structured_summary = ""
            if validated_data:
                structured_summary = "\n\n## DONNÃ‰ES NUMÃ‰RIQUES VALIDÃ‰ES (multi-sources):\n"
                for metric in validated_data[:10]:  # Top 10 mÃ©triques
                    if metric.get('coherent'):
                        structured_summary += f"- {metric['metric']}: {metric['mean']:.2f} (validÃ© par {len(metric['values'])} sources)\n"

            prompt = f"""Analyse les donnÃ©es collectÃ©es et crÃ©e un rapport structurÃ© dÃ©taillÃ©:

REQUÃŠTE: "{query}"

DONNÃ‰ES COLLECTÃ‰ES:
- {len(extracted_contents)} pages web extraites avec contenu complet
- {len(urls_info)} URLs dÃ©couvertes
- {len(all_structured_data)} sources avec donnÃ©es structurÃ©es extraites
{structured_summary}

CONTENU DES PAGES EXTRAITES (avec donnÃ©es structurÃ©es):
{json.dumps(data_for_synthesis, indent=2, ensure_ascii=False)[:60000]}

SECTIONS DEMANDÃ‰ES: {output_sections}

Ta mission: CrÃ©er un rapport complet avec TOUTES les sections demandÃ©es.

INSTRUCTIONS CRITIQUES:
1. Analyse TOUT le contenu fourni (texte + donnÃ©es structurÃ©es)
2. Pour CHAQUE section demandÃ©e, rÃ©dige 4-8 paragraphes DÃ‰TAILLÃ‰S et COMPLETS
3. **PRIORITÃ‰ AUX DONNÃ‰ES STRUCTURÃ‰ES**: Utilise numerical_data, temporal_data, entities
4. Inclus TOUS les faits prÃ©cis, chiffres, dates, noms extraits automatiquement
5. DÃ©veloppe chaque point avec contexte, implications, dÃ©tails
6. Base-toi UNIQUEMENT sur les donnÃ©es fournies
7. Cite les URLs sources utilisÃ©es pour chaque information
8. Pour chaque chiffre mentionnÃ©, ajoute-le aussi dans "data" structurÃ© de la section
9. OBJECTIF: Rapport dÃ©taillÃ© de 3-5 pages (12000-20000 caractÃ¨res minimum)

Retourne JSON:
{{
  "type": "report",
  "summary": "RÃ©sumÃ© exÃ©cutif en 2-3 phrases",
  "sections": [
    {{
      "title": "Titre section 1",
      "content": "Contenu dÃ©taillÃ© (2-4 paragraphes)",
      "data": [
        {{"metric": "nom_mÃ©trique", "value": 123.45, "unit": "unitÃ©", "source_url": "https://..."}},
        ...chiffres clÃ©s de cette section
      ],
      "sources": [{{"url": "https://...", "title": "Titre"}}]
    }},
    ...pour chaque section de {output_sections}
  ],
  "bibliography": [
    {{
      "title": "Titre de la source",
      "url": "https://...",
      "type": "website"
    }}
  ],
  "confidence": "high|medium|low"
}}

IMPORTANT: GÃ©nÃ¨re EXACTEMENT les sections: {output_sections}
"""
        else:
            # Format simple: synthÃ¨se globale
            prompt = f"""SynthÃ©tise la rÃ©ponse Ã  cette requÃªte en te basant sur les donnÃ©es collectÃ©es:

REQUÃŠTE: "{query}"

DONNÃ‰ES COLLECTÃ‰ES:
- {len(extracted_contents)} pages web extraites avec contenu complet
- {len(urls_info)} URLs dÃ©couvertes

CONTENU DES PAGES EXTRAITES:
{json.dumps(data_for_synthesis, indent=2, ensure_ascii=False)[:15000]}

Ta mission: CrÃ©er une synthÃ¨se complÃ¨te et dÃ©taillÃ©e basÃ©e sur ces donnÃ©es.

INSTRUCTIONS:
1. Analyse TOUT le contenu fourni
2. RÃ©dige une synthÃ¨se de 3-5 paragraphes minimum
3. Identifie les points clÃ©s, initiatives, chiffres importants
4. Structure l'information de maniÃ¨re logique
5. Cite les sources pertinentes

Retourne JSON:
{{
  "type": "summary",
  "summary": "SynthÃ¨se complÃ¨te et dÃ©taillÃ©e (3-5 paragraphes)",
  "key_points": ["point 1", "point 2", ...],
  "sources_used": ["url1", "url2", ...],
  "confidence": "high|medium|low"
}}
"""

        response = await self.llm_client.generate(
            [{"role": "user", "content": prompt}],
            max_tokens=15000,  # Rapports dÃ©taillÃ©s 3-5 pages
            temperature=0.1
        )

        try:
            start_idx = response.find('{')
            if start_idx != -1:
                bracket_depth = 0
                for i, char in enumerate(response[start_idx:], start=start_idx):
                    if char == '{':
                        bracket_depth += 1
                    elif char == '}':
                        bracket_depth -= 1
                        if bracket_depth == 0:
                            synthesis_result = json.loads(response[start_idx:i + 1])

                            # POST-TRAITEMENT: Extraire donnÃ©es structurÃ©es du contenu texte si rapport avec sections
                            logger.info(f"  ðŸ“‹ SynthÃ¨se: {len(synthesis_result.get('sections', []))} sections, {len(all_structured_data)} sources donnÃ©es")
                            if 'sections' in synthesis_result and len(all_structured_data) > 0:
                                logger.info(f"  ðŸ”„ Lancement post-traitement")
                                synthesis_result = self._post_process_sections(
                                    synthesis_result,
                                    all_structured_data
                                )
                            else:
                                logger.warning(f"  âš ï¸ Post-traitement ignorÃ©: sections={' sections' in synthesis_result}, data_count={len(all_structured_data)}")

                            return synthesis_result
        except Exception as e:
            logger.error(f"  âŒ Erreur parsing JSON synthÃ¨se: {str(e)}")
            logger.debug(f"  RÃ©ponse LLM (200 premiers chars): {response[:200]}")

        # Fallback: donnÃ©es brutes
        return {
            "type": "list",
            "data": all_data,
            "summary": f"{len(all_data)} items collectÃ©s",
            "confidence": "medium",
            "sources_count": len(ctx.datasets)
        }

    def _score_relevance(self, content: str, query: str, structured_data: Optional[Dict] = None) -> float:
        """
        Score la pertinence d'un chunk de contenu par rapport Ã  la requÃªte.

        Args:
            content: Contenu textuel Ã  scorer
            query: RequÃªte utilisateur
            structured_data: DonnÃ©es structurÃ©es extraites (boost le score)

        Returns:
            Score de pertinence (0-100)
        """
        if not content:
            return 0.0

        score = 0.0
        content_lower = content.lower()
        query_lower = query.lower()

        # 1. Keyword matching (max 40 points)
        query_words = set(query_lower.split())
        query_words = {w for w in query_words if len(w) > 3}  # Mots > 3 chars

        if query_words:
            matched_words = sum(1 for word in query_words if word in content_lower)
            keyword_score = min(40, (matched_words / len(query_words)) * 40)
            score += keyword_score

        # 2. Structured data boost (max 30 points)
        if structured_data:
            num_count = len(structured_data.get("numerical", []))
            temp_count = len(structured_data.get("temporal", []))
            ent_count = len(structured_data.get("entities", []))

            # Plus de donnÃ©es structurÃ©es = plus pertinent
            struct_score = min(30, (num_count * 3) + (temp_count * 2) + (ent_count * 1))
            score += struct_score

        # 3. Content length quality (max 20 points)
        # Ni trop court (< 500 chars) ni trop long (> 5000 chars)
        content_len = len(content)
        if 500 <= content_len <= 5000:
            length_score = 20
        elif content_len < 500:
            length_score = (content_len / 500) * 20
        else:  # > 5000
            length_score = max(10, 20 - ((content_len - 5000) / 1000))
        score += length_score

        # 4. Presence of numbers/dates (max 10 points)
        import re
        numbers = re.findall(r'\d+(?:[.,]\d+)?', content)
        dates = re.findall(r'\d{4}[-/]\d{2}[-/]\d{2}|\d{2}[-/]\d{2}[-/]\d{4}', content)

        data_score = min(10, len(numbers) + len(dates) * 2)
        score += data_score

        return min(100.0, score)

    def _semantic_chunk_selection(
        self,
        all_data: List[Dict],
        query: str,
        max_chars: int = 50000,
        depth: str = "moderate",
        min_score_threshold: float = None
    ) -> List[Dict]:
        """
        SÃ©lectionne intelligemment les chunks les plus pertinents selon la profondeur demandÃ©e.

        APPROCHE QUALITATIVE:
        - Au lieu de fixer une longueur arbitraire, on adapte les critÃ¨res de sÃ©lection
        - Profondeur "light": peu de chunks, seulement les plus pertinents
        - Profondeur "deep": plus de chunks, exploration large

        Args:
            all_data: Toutes les donnÃ©es collectÃ©es
            query: RequÃªte utilisateur pour contexte
            max_chars: Budget maximum de caractÃ¨res
            depth: "light" | "moderate" | "deep" - Profondeur d'analyse attendue
            min_score_threshold: Score minimum pour inclure un chunk (auto si None)

        Returns:
            Liste triÃ©e des chunks les plus pertinents
        """
        scored_items = []

        for item in all_data:
            if not isinstance(item, dict):
                continue

            content = item.get("content", "")
            if not content:
                continue

            # Score ce chunk
            structured_data = item.get("structured_data")
            score = self._score_relevance(content, query, structured_data)

            scored_items.append({
                "score": score,
                "item": item,
                "content_length": len(content)
            })

        # Trier par score dÃ©croissant
        scored_items.sort(key=lambda x: x["score"], reverse=True)

        # Adapter seuil et stratÃ©gie selon profondeur
        if min_score_threshold is None:
            if depth == "light":
                # SynthÃ¨se concise: seulement top sources trÃ¨s pertinentes
                min_score_threshold = 60.0
                max_chunks_target = 4  # Peu de sources, qualitÃ© maximale
            elif depth == "deep":
                # Analyse approfondie: exploration large
                min_score_threshold = 35.0
                max_chunks_target = 15  # Beaucoup de sources pour vision complÃ¨te
            else:  # moderate
                # Ã‰quilibrÃ© - ABAISSÃ‰ pour permettre plus de donnÃ©es
                min_score_threshold = 40.0  # Ã‰tait 55 (trop strict), maintenant 40
                max_chunks_target = 8  # Ã‰tait 6, maintenant 8
        else:
            max_chunks_target = 20  # Pas de limite si seuil custom

        # SÃ©lectionner les meilleurs chunks
        selected = []
        total_chars = 0
        chunks_selected = 0

        for scored in scored_items:
            # ArrÃªter si on a atteint le nombre cible ET que le score diminue trop
            if chunks_selected >= max_chunks_target and scored["score"] < min_score_threshold:
                break

            # Filtrer les chunks avec score trop faible
            if scored["score"] < min_score_threshold:
                continue

            item = scored["item"]
            content_len = scored["content_length"]

            # VÃ©rifier si on peut ajouter ce chunk
            if total_chars + content_len > max_chars:
                # Pour profondeur deep, essayer de prendre au moins un extrait
                if depth == "deep":
                    remaining = max_chars - total_chars
                    if remaining > 1000:  # Au moins 1000 chars disponibles
                        # Truncate ce chunk pour fit
                        item_copy = item.copy()
                        item_copy["content"] = item["content"][:remaining]
                        selected.append(item_copy)
                        total_chars += remaining
                        chunks_selected += 1
                break

            selected.append(item)
            total_chars += content_len
            chunks_selected += 1

        logger.info(f"  ðŸ“Š SÃ©lection intelligente: {len(selected)}/{len(all_data)} chunks, {total_chars:,} chars (score moyen: {sum(s['score'] for s in scored_items[:len(selected)]) / max(len(selected), 1):.1f}/100)")

        return selected

    def _post_process_sections(
        self,
        synthesis_result: Dict[str, Any],
        all_structured_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Post-traite les sections pour extraire les donnÃ©es structurÃ©es du texte.
        """
        import re

        logger.info(f"  ðŸ”„ POST-TRAITEMENT: DÃ©marrage avec {len(all_structured_data)} sources de donnÃ©es")

        # AgrÃ©ger toutes les donnÃ©es numÃ©riques
        all_numerical = []
        for struct_data in all_structured_data:
            if isinstance(struct_data, dict) and "numerical" in struct_data:
                all_numerical.extend(struct_data.get("numerical", []))

        # CrÃ©er mapping: mÃ©trique â†’ donnÃ©es
        metrics_map = {}
        for num_data in all_numerical:
            if isinstance(num_data, dict):
                metric = num_data.get("metric", "").lower()
                if metric:
                    if metric not in metrics_map:
                        metrics_map[metric] = []
                    metrics_map[metric].append(num_data)

        # Patterns pour dÃ©tecter chiffres dans texte
        number_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*(%|â‚¬|millions?|milliards?|K|M|Md)',
            r'(\d+(?:[.,]\d+)?)\s+([A-Za-zÃ€-Ã¿]+)',
        ]

        for section in synthesis_result.get('sections', []):
            existing_data = section.get('data')
            if existing_data is not None and len(existing_data) > 0:
                continue  # DÃ©jÃ  rempli par le LLM

            content = section.get('content', '')
            section_data = []

            for pattern in number_patterns:
                matches = re.finditer(pattern, content)
                for match in matches:
                    try:
                        value_str = match.group(1).replace(',', '.')
                        value = float(value_str)
                        unit = match.group(2) if len(match.groups()) > 1 else None

                        context_start = max(0, match.start() - 20)
                        context = content[context_start:match.start()].strip()

                        source_url = None
                        metric_name = None

                        for metric, num_list in metrics_map.items():
                            for num in num_list:
                                if abs(num.get("value", 0) - value) < 0.01:
                                    if num.get("unit") == unit or unit is None:
                                        source_url = num.get("source_url")
                                        metric_name = num.get("metric")
                                        break
                            if source_url:
                                break

                        section_data.append({
                            "metric": metric_name or "valeur",
                            "value": value,
                            "unit": unit,
                            "source_url": source_url
                        })

                    except (ValueError, IndexError):
                        continue

            if section_data:
                section['data'] = section_data[:5]
                logger.info(f"  âœ… Post-traitement: {len(section_data[:5])} donnÃ©es pour '{section.get('title')}'")

        return synthesis_result

    async def _iterative_enrichment(
        self,
        query: str,
        ctx: ExecutionContext,
        report: Dict[str, Any],
        max_iterations: int = 2
    ) -> Dict[str, Any]:
        """
        Enrichissement itÃ©ratif du rapport:
        1. Analyse qualitÃ© de chaque section
        2. DÃ©tecte les manques (chiffres, prÃ©cisions, cohÃ©rence)
        3. Relance recherches ciblÃ©es si nÃ©cessaire
        4. RÃ©gÃ©nÃ¨re sections faibles
        """

        for iteration in range(max_iterations):
            logger.info(f"  ðŸ”„ ItÃ©ration {iteration + 1}/{max_iterations}")

            # Analyser qualitÃ© de chaque section
            quality_analysis = await self._analyze_report_quality(query, report, ctx)

            logger.info(f"  ðŸ“Š QualitÃ© globale: {quality_analysis.get('overall_score', 0)}/100")

            # Si qualitÃ© suffisante, arrÃªter
            if quality_analysis.get('overall_score', 0) >= 85:
                logger.info("  âœ… QualitÃ© suffisante atteinte")
                break

            # Identifier sections Ã  enrichir (score < 75 OU longueur < 2000 chars)
            weak_sections = []
            for i, section_analysis in enumerate(quality_analysis.get('sections_analysis', [])):
                section_title = section_analysis.get('title')
                score = section_analysis.get('score', 100)

                # Trouver la section correspondante dans le rapport
                actual_section = None
                for sec in report.get('sections', []):
                    if sec.get('title') == section_title:
                        actual_section = sec
                        break

                # CritÃ¨res: score < 75 OU longueur < 2000 chars
                content_length = len(actual_section.get('content', '')) if actual_section else 0
                if score < 75 or content_length < 2000:
                    weak_sections.append(section_analysis)
                    if content_length < 2000:
                        logger.info(f"  ðŸ“ Section '{section_title}' trop courte: {content_length} chars")

            if not weak_sections:
                logger.info("  âœ… Toutes les sections sont de qualitÃ© et longueur suffisantes")
                break

            logger.info(f"  ðŸŽ¯ {len(weak_sections)} section(s) Ã  enrichir")

            # Enrichir les sections faibles (max 2 par itÃ©ration)
            for section_analysis in weak_sections[:2]:
                section_title = section_analysis.get('title')
                missing = section_analysis.get('missing', [])

                logger.info(f"  ðŸ” Enrichissement: {section_title}")
                logger.info(f"    Manques: {', '.join(missing)}")

                # Recherche ciblÃ©e pour combler les manques
                enrichment_data = await self._targeted_search(query, section_title, missing, ctx)

                # RÃ©gÃ©nÃ©rer la section avec les nouvelles donnÃ©es
                if enrichment_data:
                    updated_section = await self._regenerate_section(
                        query, section_title, report, enrichment_data, ctx
                    )

                    # Remplacer la section dans le rapport
                    for i, section in enumerate(report.get('sections', [])):
                        if section.get('title') == section_title:
                            report['sections'][i] = updated_section
                            break

        return report

    async def _analyze_report_quality(
        self,
        query: str,
        report: Dict[str, Any],
        ctx: ExecutionContext
    ) -> Dict[str, Any]:
        """Analyse la qualitÃ© de chaque section du rapport."""

        sections_text = []
        for section in report.get('sections', []):
            sections_text.append(f"## {section.get('title')}\n{section.get('content', '')}")

        prompt = f"""Analyse la qualitÃ© de ce rapport de recherche:

REQUÃŠTE: "{query}"

RAPPORT:
{chr(10).join(sections_text)[:5000]}

Pour CHAQUE section, Ã©value:

1. **PrÃ©sence de donnÃ©es factuelles**: Chiffres, dates, noms, statistiques
2. **PrÃ©cision**: DÃ©tails concrets vs gÃ©nÃ©ralitÃ©s
3. **CohÃ©rence**: Logique et liens entre informations
4. **ComplÃ©tude**: RÃ©pond-elle pleinement Ã  son objectif?

Retourne JSON:
{{
  "overall_score": 0-100,
  "sections_analysis": [
    {{
      "title": "Nom section",
      "score": 0-100,
      "missing": ["chiffres prÃ©cis", "dates", "noms d'initiatives", "donnÃ©es quantitatives"],
      "strengths": ["points forts"],
      "weaknesses": ["points faibles"]
    }}
  ],
  "gaps_to_fill": [
    {{
      "section": "Nom section",
      "gap": "Description du manque",
      "search_needed": "RequÃªte de recherche suggÃ©rÃ©e"
    }}
  ]
}}

CRITÃˆRES:
- Score 90-100: Excellent, dÃ©taillÃ©, chiffrÃ©, sourcÃ©
- Score 75-89: Bon mais peut Ãªtre enrichi
- Score 50-74: Manque de prÃ©cision/chiffres
- Score <50: Trop gÃ©nÃ©rique, nÃ©cessite enrichissement
"""

        try:
            response = await self.llm_client.generate(
                [{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0.1
            )

            # Parser JSON
            start_idx = response.find('{')
            if start_idx != -1:
                bracket_depth = 0
                for i, char in enumerate(response[start_idx:], start=start_idx):
                    if char == '{':
                        bracket_depth += 1
                    elif char == '}':
                        bracket_depth -= 1
                        if bracket_depth == 0:
                            return json.loads(response[start_idx:i + 1])
        except Exception as e:
            logger.error(f"Erreur analyse qualitÃ©: {e}")

        # Fallback
        return {
            "overall_score": 70,
            "sections_analysis": [],
            "gaps_to_fill": []
        }

    async def _targeted_search(
        self,
        query: str,
        section_title: str,
        missing: List[str],
        ctx: ExecutionContext
    ) -> Dict[str, Any]:
        """Recherche ciblÃ©e pour combler les manques d'une section."""

        # Construire une requÃªte spÃ©cifique pour les manques
        search_query = f"{query} {section_title} {' '.join(missing[:3])}"

        logger.info(f"    ðŸ”Ž Recherche ciblÃ©e: {search_query}")

        try:
            # Recherche SearXNG
            results = await searxng_client.search(search_query, max_results=5)

            if not results:
                return None

            # Extraire les 2 premiers rÃ©sultats les plus pertinents
            urls_to_extract = [r.url for r in results[:2]]

            extracted_data = []
            for url in urls_to_extract:
                try:
                    extract_result = await self.extractor_manager.extract(
                        url=url,
                        llm_client=self.llm_client,
                        options=ExtractionOptions(
                            timeout=30,
                            use_agent=False,
                            headless=True
                        )
                    )

                    if extract_result.success and extract_result.content:
                        # Extraction donnÃ©es structurÃ©es (enrichissement ciblÃ©)
                        structured = await self.data_extractor.extract_structured_data(
                            content=extract_result.content[:2000],
                            source_url=url,
                            topic_context=f"{query} - {section_title}"
                        )

                        extracted_data.append({
                            "url": url,
                            "title": extract_result.title or "",
                            "content": extract_result.content[:2000],
                            "structured_data": structured.to_dict()
                        })
                        # Ajouter aux sources dÃ©couvertes (avec dÃ©duplication)
                        if url not in ctx.discovered_sources:
                            ctx.discovered_sources.append(url)
                except Exception as e:
                    logger.warning(f"    âš ï¸ Ã‰chec extraction {url}: {e}")
                    continue

            return {
                "query": search_query,
                "sources": extracted_data,
                "missing": missing
            }

        except Exception as e:
            logger.error(f"Erreur recherche ciblÃ©e: {e}")
            return None

    async def _regenerate_section(
        self,
        query: str,
        section_title: str,
        report: Dict[str, Any],
        enrichment_data: Dict[str, Any],
        ctx: ExecutionContext
    ) -> Dict[str, Any]:
        """RÃ©gÃ©nÃ¨re une section avec les donnÃ©es enrichies."""

        # RÃ©cupÃ©rer la section actuelle
        current_section = None
        for section in report.get('sections', []):
            if section.get('title') == section_title:
                current_section = section
                break

        if not current_section:
            return None

        # DonnÃ©es d'enrichissement
        enrichment_content = json.dumps(
            enrichment_data.get('sources', []),
            indent=2,
            ensure_ascii=False
        )[:3000]

        prompt = f"""AmÃ©liore cette section de rapport avec les nouvelles donnÃ©es collectÃ©es:

REQUÃŠTE GLOBALE: "{query}"

SECTION Ã€ AMÃ‰LIORER: {section_title}

CONTENU ACTUEL:
{current_section.get('content', '')}

NOUVELLES DONNÃ‰ES COLLECTÃ‰ES:
{enrichment_content}

MANQUES IDENTIFIÃ‰S: {', '.join(enrichment_data.get('missing', []))}

Ta mission:
1. ENRICHIR le contenu existant avec les nouvelles donnÃ©es
2. AJOUTER des chiffres prÃ©cis, dates, noms trouvÃ©s
3. MAINTENIR la cohÃ©rence avec le reste du rapport
4. CITER les nouvelles sources utilisÃ©es

Retourne JSON:
{{
  "title": "{section_title}",
  "content": "Contenu enrichi (2-5 paragraphes avec dÃ©tails factuels)",
  "sources": [{{"url": "...", "title": "..."}}]
}}

IMPORTANT: Le contenu doit Ãªtre PLUS dÃ©taillÃ© et factuel que l'original.
"""

        try:
            response = await self.llm_client.generate(
                [{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0.1
            )

            # Parser JSON
            start_idx = response.find('{')
            if start_idx != -1:
                bracket_depth = 0
                for i, char in enumerate(response[start_idx:], start=start_idx):
                    if char == '{':
                        bracket_depth += 1
                    elif char == '}':
                        bracket_depth -= 1
                        if bracket_depth == 0:
                            result = json.loads(response[start_idx:i + 1])
                            logger.info(f"    âœ… Section enrichie: {section_title}")
                            return result
        except Exception as e:
            logger.error(f"Erreur rÃ©gÃ©nÃ©ration section: {e}")

        # Fallback: retourner la section actuelle
        return current_section

    async def _validate_numerical_coherence(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """Valide la cohÃ©rence des chiffres mentionnÃ©s dans le rapport."""

        sections_text = []
        for section in report.get('sections', []):
            sections_text.append(f"## {section.get('title')}\n{section.get('content', '')}")

        full_text = "\n\n".join(sections_text)

        prompt = f"""Analyse la cohÃ©rence numÃ©rique de ce rapport:

{full_text[:8000]}

Ta mission:
1. Extraire TOUS les chiffres mentionnÃ©s (montants, pourcentages, dates, quantitÃ©s)
2. VÃ©rifier leur cohÃ©rence temporelle et logique
3. Identifier les incohÃ©rences potentielles
4. Proposer des corrections si nÃ©cessaire

Retourne JSON:
{{
  "coherent": true|false,
  "issues": [
    {{
      "type": "temporal_incoherence|logical_contradiction|missing_unit",
      "description": "Description du problÃ¨me",
      "values_involved": ["7.1 milliards en 2024", "13 milliards total"],
      "severity": "high|medium|low",
      "suggested_fix": "Explication ou correction"
    }}
  ],
  "key_numbers": [
    {{
      "value": "7.1",
      "unit": "milliards euros",
      "context": "levÃ©es 2024",
      "temporal_marker": "2024"
    }}
  ]
}}

RÃˆGLES:
- Si total cumulÃ© < montant annuel rÃ©cent: incohÃ©rence temporelle
- Si pourcentages ne somment pas correctement: incohÃ©rence logique
- Si dates contradictoires: incohÃ©rence temporelle
"""

        try:
            response = await self.llm_client.generate(
                [{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.1
            )

            # Parser JSON
            start_idx = response.find('{')
            if start_idx != -1:
                bracket_depth = 0
                for i, char in enumerate(response[start_idx:], start=start_idx):
                    if char == '{':
                        bracket_depth += 1
                    elif char == '}':
                        bracket_depth -= 1
                        if bracket_depth == 0:
                            validation = json.loads(response[start_idx:i + 1])

                            # Logger les problÃ¨mes trouvÃ©s
                            if not validation.get('coherent', True):
                                high_severity_issues = [
                                    issue for issue in validation.get('issues', [])
                                    if issue.get('severity') == 'high'
                                ]
                                if high_severity_issues:
                                    logger.warning(f"âš ï¸ {len(high_severity_issues)} incohÃ©rence(s) numÃ©rique(s) dÃ©tectÃ©e(s)")
                                    for issue in high_severity_issues:
                                        logger.warning(f"  - {issue.get('description')}")
                                        logger.info(f"    Correction suggÃ©rÃ©e: {issue.get('suggested_fix')}")

                                # Ajouter une note de validation au rapport
                                report['validation'] = {
                                    'numerical_coherence_checked': True,
                                    'issues_found': len(validation.get('issues', [])),
                                    'high_severity_issues': len(high_severity_issues)
                                }
                            else:
                                logger.info("âœ“ CohÃ©rence numÃ©rique validÃ©e")
                                report['validation'] = {
                                    'numerical_coherence_checked': True,
                                    'coherent': True
                                }

                            return report
        except Exception as e:
            logger.error(f"Erreur validation cohÃ©rence: {e}")

        # Fallback: retourner sans modification
        return report

    async def close(self):
        """Ferme les ressources."""
        await self.adaptive_navigator.close()
    # ===================================================================
    # NOUVELLE ARCHITECTURE OPTIMISÃ‰E
    # Phase 3: SynthÃ¨se par section
    # Phase 4: Assemblage global et Ã©valuation
    # Phase 5: Comblement des manques
    # ===================================================================

    async def _synthesize_sections(self, query: str, ctx: ExecutionContext) -> Dict[str, Any]:
        """
        PHASE 3: SynthÃ¨se par section.
        Pour chaque section, synthÃ©tise les donnÃ©es brutes collectÃ©es en contenu rÃ©digÃ©.
        """
        logger.info(f"ðŸ“ PHASE 3: SynthÃ¨se par section ({len(ctx.final_content['sections'])} sections)")

        for section_name, section_data in ctx.final_content['sections'].items():
            raw_data = section_data['raw_data']

            if not raw_data:
                logger.warning(f"  âš ï¸ Section '{section_name}': aucune donnÃ©e")
                continue

            logger.info(f"  ðŸ”„ SynthÃ¨se section: {section_name} ({len(raw_data)} donnÃ©es)")

            # DÃ©terminer la profondeur d'analyse pour cette section
            # TODO: Extraire du plan gÃ©nÃ©rÃ© en Phase 1 section_targets
            # Pour l'instant, utiliser moderate par dÃ©faut
            section_depth = "moderate"

            # SÃ©lection intelligente des donnÃ©es pertinentes pour cette section
            selected_data = self._semantic_chunk_selection(raw_data, query, max_chars=20000, depth=section_depth)

            # PrÃ©paration du prompt de synthÃ¨se
            data_for_synthesis = []
            all_structured_data = []

            for item in selected_data:
                if isinstance(item, dict):
                    if "content" in item and item.get("content"):
                        page_data = {
                            "url": item.get("url", ""),
                            "title": item.get("title", ""),
                            "content": item.get("content", "")
                        }

                        # Ajouter donnÃ©es structurÃ©es
                        if "structured_data" in item and item["structured_data"]:
                            struct = item["structured_data"]
                            page_data["numerical_data"] = struct.get("numerical", [])
                            page_data["temporal_data"] = struct.get("temporal", [])
                            page_data["entities"] = struct.get("entities", [])
                            all_structured_data.append(struct)

                        data_for_synthesis.append(page_data)

            # Appel LLM pour synthÃ©tiser cette section
            # APPROCHE QUALITATIVE: adapter les instructions selon la profondeur, pas imposer longueur arbitraire
            if section_depth == "light":
                synthesis_instructions = """Ta mission: SynthÃ¨se CONCISE des points clÃ©s uniquement.

INSTRUCTIONS:
1. Identifie les 3-5 informations les plus importantes
2. RÃ©dige 2-3 paragraphes courts et prÃ©cis
3. Va droit Ã  l'essentiel, sans dÃ©tails secondaires
4. Cite les sources principales"""
            elif section_depth == "deep":
                synthesis_instructions = """Ta mission: Analyse APPROFONDIE explorant tous les aspects.

INSTRUCTIONS:
1. Analyse TOUTES les donnÃ©es fournies en profondeur
2. Explore les interconnexions et implications
3. RÃ©dige 6-10 paragraphes dÃ©taillÃ©s
4. DÃ©veloppe les nuances et contextes
5. Compare les sources et perspectives
6. Cite systÃ©matiquement toutes les sources"""
            else:  # moderate
                synthesis_instructions = """Ta mission: RÃ©diger un contenu Ã‰QUILIBRÃ‰ et informatif.

INSTRUCTIONS:
1. Analyse les donnÃ©es principales
2. RÃ©dige 4-6 paragraphes structurÃ©s
3. Ã‰quilibre profondeur et clartÃ©
4. Cite les sources importantes"""

            prompt = f"""SynthÃ©tise le contenu pour la section "{section_name}" du rapport.

REQUÃŠTE INITIALE: "{query}"

DONNÃ‰ES DISPONIBLES ({len(data_for_synthesis)} sources):
{json.dumps(data_for_synthesis, indent=2, ensure_ascii=False)[:25000]}

DONNÃ‰ES STRUCTURÃ‰ES EXTRAITES:
- {sum(len(s.get('numerical', [])) for s in all_structured_data)} donnÃ©es numÃ©riques
- {sum(len(s.get('temporal', [])) for s in all_structured_data)} donnÃ©es temporelles
- {sum(len(s.get('entities', [])) for s in all_structured_data)} entitÃ©s

{synthesis_instructions}

RÃˆGLES TECHNIQUES:
- CITE LES SOURCES: Pour chaque affirmation, ajoute [SOURCE:url_exacte]
  Exemple: "Selon l'Ã©tude de 2024 [SOURCE:https://exemple.com], l'adoption a augmentÃ© de 45%."
- Utilise les donnÃ©es structurÃ©es (chiffres, dates, noms) avec prÃ©cision
- INTERDICTION: Ne PAS inventer de rÃ©fÃ©rences [1], [2], [3] - utilise UNIQUEMENT [SOURCE:url]
- La longueur sera la CONSÃ‰QUENCE NATURELLE de la qualitÃ© de l'analyse, pas un objectif arbitraire

Retourne JSON:
{{
  "content": "Contenu rÃ©digÃ© avec citations [SOURCE:url]...",
  "key_data": [
    {{"metric": "...", "value": ..., "unit": "...", "source": "url"}}
  ],
  "sources_used": ["url1", "url2", "url3"]
}}

RAPPEL: Chaque affirmation factuelle DOIT avoir [SOURCE:url]. Les sources_used doivent lister TOUTES les URLs citÃ©es.

RÃ©dige maintenant pour "{section_name}":"""

            response = await self.llm_client.generate(
                [{"role": "user", "content": prompt}],
                max_tokens=5000,
                temperature=0.3
            )

            # Parser JSON
            try:
                start_idx = response.find('{')
                if start_idx != -1:
                    bracket_depth = 0
                    for i, char in enumerate(response[start_idx:], start=start_idx):
                        if char == '{':
                            bracket_depth += 1
                        elif char == '}':
                            bracket_depth -= 1
                            if bracket_depth == 0:
                                section_result = json.loads(response[start_idx:i + 1])

                                # Mettre Ã  jour la section dans le contexte
                                ctx.final_content['sections'][section_name]['content'] = section_result.get('content', '')
                                ctx.final_content['sections'][section_name]['metadata']['key_data'] = section_result.get('key_data', [])
                                ctx.final_content['sections'][section_name]['metadata']['sources'] = section_result.get('sources_used', [])

                                logger.info(f"    âœ“ Section synthÃ©tisÃ©e: {len(section_result.get('content', ''))} chars")
                                break
            except Exception as e:
                logger.error(f"  âŒ Erreur synthÃ¨se section '{section_name}': {e}")

        return ctx.final_content

    async def _assemble_and_evaluate(self, query: str, ctx: ExecutionContext) -> Dict[str, Any]:
        """
        PHASE 4: Assemblage global et Ã©valuation.
        Assemble les sections, crÃ©e cohÃ©rence inter-sections, identifie manques.
        """
        logger.info("ðŸ”— PHASE 4: Assemblage global et Ã©valuation")

        # PrÃ©parer les sections pour l'assemblage
        sections_content = []
        for section_name, section_data in ctx.final_content['sections'].items():
            if section_data['content']:
                sections_content.append({
                    "title": section_name,
                    "content": section_data['content'],
                    "data": section_data['metadata'].get('key_data', [])
                })

        if not sections_content:
            return {
                "type": "error",
                "message": "Aucune section synthÃ©tisÃ©e"
            }

        # Assemblage et mise en cohÃ©rence
        prompt = f"""Assemble et finalise ce rapport.

REQUÃŠTE: "{query}"

SECTIONS DISPONIBLES ({len(sections_content)}):
{json.dumps(sections_content, indent=2, ensure_ascii=False)[:30000]}

Ta mission:
1. VÃ©rifier cohÃ©rence entre sections
2. Ajouter transitions/liaisons SANS supprimer le contenu existant
3. ComplÃ©ter introduction/conclusion si nÃ©cessaire
4. VÃ©rifier cohÃ©rence numÃ©rique

RÃˆGLES CRITIQUES:
- PRÃ‰SERVER TOUTES les citations [SOURCE:url] prÃ©sentes dans le contenu
- NE PAS rÃ©Ã©crire ou reformuler les sections - seulement ajouter transitions
- CONSERVER toutes les donnÃ©es structurÃ©es (data)
- Si tu ajoutes des transitions, utilise aussi des [SOURCE:url] si nÃ©cessaire

Retourne JSON:
{{
  "type": "report",
  "title": "Titre du rapport",
  "sections": [
    {{
      "title": "...",
      "content": "Contenu PRÃ‰SERVÃ‰ avec [SOURCE:url] + transitions ajoutÃ©es",
      "data": [...]  // TOUS les data de la section originale
    }}
  ],
  "metadata": {{
    "word_count": ...,
    "coherence_score": 0-100,
    "sources_count": ...
  }}
}}"""

        response = await self.llm_client.generate(
            [{"role": "user", "content": prompt}],
            max_tokens=10000,
            temperature=0.2
        )

        # Parser JSON
        try:
            start_idx = response.find('{')
            if start_idx != -1:
                bracket_depth = 0
                for i, char in enumerate(response[start_idx:], start=start_idx):
                    if char == '{':
                        bracket_depth += 1
                    elif char == '}':
                        bracket_depth -= 1
                        if bracket_depth == 0:
                            final_report = json.loads(response[start_idx:i + 1])
                            logger.info(f"  âœ“ Rapport assemblÃ©: {len(final_report.get('sections', []))} sections")
                            return final_report
        except Exception as e:
            logger.error(f"  âŒ Erreur assemblage: {e}")

        # Fallback: retourner sections brutes
        return {
            "type": "report",
            "sections": sections_content,
            "metadata": {"coherence_score": 70}
        }

    def _convert_sources_to_bibliography(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """
        Convertit les citations [SOURCE:url] en rÃ©fÃ©rences numÃ©rotÃ©es [1], [2], etc.
        et gÃ©nÃ¨re une bibliographie ordonnÃ©e.

        AMÃ‰LIORATION: Inclut aussi TOUTES les sources des donnÃ©es structurÃ©es,
        mÃªme si elles ne sont pas citÃ©es explicitement dans le texte.
        """
        import re

        # Collecter toutes les URLs uniques citÃ©es dans le rapport
        url_to_num = {}
        bibliography = []
        counter = 1

        # Ã‰TAPE 1: Collecter les URLs citÃ©es dans le contenu avec [SOURCE:url]
        for section in report.get('sections', []):
            content = section.get('content', '')

            # Trouver toutes les citations [SOURCE:url]
            pattern = r'\[SOURCE:(https?://[^\]]+)\]'
            matches = re.findall(pattern, content)

            for url in matches:
                if url not in url_to_num:
                    url_to_num[url] = counter
                    # Extraire le domaine pour le titre
                    domain = url.split('/')[2] if len(url.split('/')) > 2 else url
                    bibliography.append({
                        "id": counter,
                        "url": url,
                        "title": f"Source {counter} - {domain}",
                        "accessed": datetime.now().strftime("%Y-%m-%d")
                    })
                    counter += 1

        # Ã‰TAPE 2: Ajouter les sources des donnÃ©es structurÃ©es (mÃ©triques, etc.)
        # Ces sources ont Ã©tÃ© utilisÃ©es mÃªme si non citÃ©es explicitement dans le texte
        for section in report.get('sections', []):
            data_points = section.get('data', [])
            for data_point in data_points:
                source_url = data_point.get('source')
                if source_url and source_url not in url_to_num:
                    url_to_num[source_url] = counter
                    domain = source_url.split('/')[2] if len(source_url.split('/')) > 2 else source_url
                    bibliography.append({
                        "id": counter,
                        "url": source_url,
                        "title": f"Source {counter} - {domain}",
                        "accessed": datetime.now().strftime("%Y-%m-%d")
                    })
                    counter += 1

        # Remplacer [SOURCE:url] par [numÃ©ro] dans toutes les sections
        for section in report.get('sections', []):
            content = section.get('content', '')

            # Remplacer chaque citation par son numÃ©ro
            for url, num in url_to_num.items():
                content = content.replace(f'[SOURCE:{url}]', f'[{num}]')

            section['content'] = content

        # Ajouter la bibliographie au rapport
        report['bibliography'] = bibliography

        logger.info(f"  âœ“ Bibliographie gÃ©nÃ©rÃ©e: {len(bibliography)} sources")

        return report

    def _identify_gaps(self, report: Dict[str, Any], ctx: ExecutionContext) -> List[Dict[str, Any]]:
        """
        Identifie les manques dans le rapport SANS appel LLM.
        Utilise des heuristiques simples.
        """
        gaps = []

        for section in report.get('sections', []):
            section_title = section.get('title', '')
            content = section.get('content', '')
            data = section.get('data', [])

            # CritÃ¨re 1: Section trop courte
            if len(content) < 1500:
                gaps.append({
                    "section": section_title,
                    "type": "content_too_short",
                    "description": f"Section courte: {len(content)} chars",
                    "priority": "high" if len(content) < 800 else "medium"
                })

            # CritÃ¨re 2: Pas assez de donnÃ©es structurÃ©es
            if len(data) < 2:
                gaps.append({
                    "section": section_title,
                    "type": "missing_data",
                    "description": f"Peu de donnÃ©es: {len(data)} mÃ©triques",
                    "priority": "medium"
                })

            # CritÃ¨re 3: Pas de sources citÃ©es
            if "http" not in content and "www" not in content:
                gaps.append({
                    "section": section_title,
                    "type": "missing_sources",
                    "description": "Aucune source citÃ©e",
                    "priority": "low"
                })

        logger.info(f"  ðŸ“Š Manques identifiÃ©s: {len(gaps)}")
        for gap in gaps:
            logger.info(f"    - {gap['section']}: {gap['description']} ({gap['priority']})")

        return gaps

    async def _fill_gaps_iteration(
        self,
        query: str,
        ctx: ExecutionContext,
        report: Dict[str, Any],
        gaps: List[Dict],
        max_iterations: int = 1
    ) -> Dict[str, Any]:
        """
        PHASE 5: Comblement des manques par itÃ©ration.
        Relance recherches ciblÃ©es et re-synthÃ¨se.
        """
        if not gaps:
            return report

        logger.info(f"ðŸ”„ PHASE 5: Comblement de {len(gaps)} manques (max {max_iterations} itÃ©rations)")

        # Filtrer les gaps prioritaires
        high_priority_gaps = [g for g in gaps if g['priority'] == 'high']
        if not high_priority_gaps:
            logger.info("  âœ“ Pas de manques critiques, rapport suffisant")
            return report

        for iteration in range(max_iterations):
            logger.info(f"  ðŸ”„ ItÃ©ration {iteration + 1}/{max_iterations}")

            # Traiter les 2 premiers gaps prioritaires
            for gap in high_priority_gaps[:2]:
                section_name = gap['section']
                gap_type = gap['type']

                logger.info(f"    ðŸŽ¯ Comblement: {section_name} - {gap_type}")

                # Recherche ciblÃ©e
                search_query = f"{query} {section_name}"
                enrichment_data = await self._targeted_search(
                    query=query,
                    section_title=section_name,
                    missing_aspects=[gap_type],
                    ctx=ctx
                )

                if enrichment_data:
                    # Ajouter aux raw_data de la section
                    if section_name in ctx.final_content['sections']:
                        ctx.final_content['sections'][section_name]['raw_data'].extend(enrichment_data)

            # Re-synthÃ¨se des sections modifiÃ©es
            await self._synthesize_sections(query, ctx)

            # Re-assemblage
            report = await self._assemble_and_evaluate(query, ctx)

            # Re-Ã©valuation des gaps
            new_gaps = self._identify_gaps(report, ctx)
            if len(new_gaps) < len(high_priority_gaps):
                logger.info(f"  âœ“ ProgrÃ¨s: {len(high_priority_gaps)} â†’ {len(new_gaps)} manques")
                high_priority_gaps = [g for g in new_gaps if g['priority'] == 'high']
                if not high_priority_gaps:
                    break
            else:
                logger.info("  âš ï¸ Pas d'amÃ©lioration, arrÃªt")
                break

        return report

    # ========================================================================
    # NOUVELLES MÃ‰THODES POUR ARCHITECTURE REFONDÃ‰E
    # ========================================================================

    async def _exploratory_phase(self, query: str, context: Dict) -> Dict[str, Any]:
        """
        PHASE 1.1: Exploration initiale restreinte pour Ã©valuer le champ.

        But: DÃ©couvrir rapidement les principales sources disponibles,
        identifier les directions possibles, Ã©valuer la richesse du sujet.

        Retourne:
        - sources: Liste des URLs dÃ©couvertes (5-10 max)
        - field_assessment: Analyse du champ (rich/moderate/limited)
        - topics_found: Sous-thÃ¨mes dÃ©couverts
        - data_preview: AperÃ§u rapide du contenu
        """
        logger.info("  ðŸ”Ž Exploration initiale (recherche restreinte)")

        # Recherche exploratoire avec limite rÃ©duite
        search_params = {
            "query": query,
            "max_results": 8,  # Restreint pour exploration rapide
            "categories": ["general"],
            "engines": ["google", "duckduckgo"]
        }

        try:
            search_result = await searxng_client.search(
                query=search_params["query"],
                categories=search_params["categories"],
                engines=search_params["engines"]
            )

            urls_found = []
            if search_result.get("results"):
                urls_found = [
                    {
                        "url": r.get("url"),
                        "title": r.get("title", ""),
                        "snippet": r.get("content", "")[:200]
                    }
                    for r in search_result["results"][:search_params["max_results"]]
                ]

            # Analyser les snippets pour identifier sous-thÃ¨mes
            topics_found = self._extract_topics_from_snippets(urls_found)

            # Ã‰valuer la richesse du champ
            field_assessment = "rich" if len(urls_found) >= 6 else ("moderate" if len(urls_found) >= 3 else "limited")

            logger.info(f"     â†’ {len(urls_found)} sources, champ: {field_assessment}")
            logger.info(f"     â†’ Sous-thÃ¨mes: {', '.join(topics_found[:3])}")

            return {
                "sources": urls_found,
                "field_assessment": field_assessment,
                "topics_found": topics_found,
                "data_preview": {
                    "total_sources": len(urls_found),
                    "snippets": [u["snippet"] for u in urls_found[:3]]
                }
            }

        except Exception as e:
            logger.error(f"  âŒ Erreur exploration: {e}")
            return {
                "sources": [],
                "field_assessment": "unknown",
                "topics_found": [],
                "data_preview": {}
            }

    def _extract_topics_from_snippets(self, urls_data: List[Dict]) -> List[str]:
        """Extrait les sous-thÃ¨mes des snippets de recherche."""
        # Analyse simple par mots-clÃ©s frÃ©quents
        from collections import Counter
        import re

        words = []
        for url_data in urls_data:
            text = f"{url_data.get('title', '')} {url_data.get('snippet', '')}"
            # Extraire mots de 4+ lettres
            words.extend(re.findall(r'\b[a-zA-Z]{4,}\b', text.lower()))

        # Mots les plus frÃ©quents (hors stopwords basiques)
        stopwords = {'this', 'that', 'with', 'from', 'have', 'more', 'will', 'your', 'about', 'what', 'when', 'where', 'which', 'their', 'there'}
        filtered = [w for w in words if w not in stopwords]
        common = Counter(filtered).most_common(5)

        return [word for word, count in common if count >= 2]

    async def _create_detailed_plan(
        self,
        query: str,
        context: Dict,
        exploration_data: Dict
    ) -> Dict[str, Any]:
        """
        PHASE 1.2: Planification dÃ©taillÃ©e avec canvas complet.

        Utilise les donnÃ©es d'exploration pour crÃ©er un plan prÃ©cis incluant:
        - Structure complÃ¨te (sections, sous-sections)
        - EnchaÃ®nements narratifs
        - Objectifs par section
        - Profondeur adaptÃ©e
        """
        logger.info("  ðŸ“‹ CrÃ©ation du plan dÃ©taillÃ© avec canvas")

        # Construire le contexte enrichi
        topics_found = exploration_data.get("topics_found", [])
        field_richness = exploration_data.get("field_assessment", "moderate")
        sources_count = len(exploration_data.get("sources", []))

        prompt = f"""Tu es un planificateur expert. CrÃ©e un plan dÃ©taillÃ© pour rÃ©pondre Ã  cette requÃªte.

REQUÃŠTE: "{query}"

DONNÃ‰ES D'EXPLORATION:
- Sources dÃ©couvertes: {sources_count}
- Richesse du champ: {field_richness}
- Sous-thÃ¨mes identifiÃ©s: {', '.join(topics_found[:5])}
- AperÃ§u: {exploration_data.get('data_preview', {}).get('snippets', [])[0][:150] if exploration_data.get('data_preview', {}).get('snippets') else 'N/A'}

CONTEXTE UTILISATEUR:
{json.dumps(context, indent=2) if context else "Aucun"}

Ta mission: CrÃ©er un CANVAS DÃ‰TAILLÃ‰ avec structure complÃ¨te et enchaÃ®nements.

Ã‰TAPE 1: ANALYSE MULTI-CRITÃˆRES
Ã‰value la complexitÃ© nÃ©cessaire (1-5 pour chaque critÃ¨re):

1. **ComplexitÃ© du sujet**: Simple (1) â†’ Multidimensionnel (5)
2. **SpÃ©cificitÃ© demandÃ©e**: Large (1) â†’ TrÃ¨s prÃ©cis (5)
3. **Format**: RÃ©sumÃ© (1) â†’ Ã‰tude approfondie (5)
4. **Profondeur temporelle**: Point dans le temps (1) â†’ Ã‰volution + projection (5)
5. **Interconnexions**: Sujet isolÃ© (1) â†’ Analyse systÃ©mique (5)

Score moyen = profondeur globale

Ã‰TAPE 2: DÃ‰TERMINER AMPLEUR ET STRUCTURE

Selon score_profondeur:
- 1.0-2.0: Rapport CONCIS (1-2 sections, 500-1000 mots)
- 2.1-3.0: Rapport STANDARD (2-3 sections, 1000-1500 mots)
- 3.1-4.0: Rapport DÃ‰TAILLÃ‰ (3-5 sections, 1500-2500 mots)
- 4.1-5.0: Ã‰tude APPROFONDIE (4-7 sections, 2500-4000 mots)

Ã‰TAPE 3: DÃ‰FINIR ENCHAÃŽNEMENTS NARRATIFS

Pour chaque transition entre sections, dÃ©finis:
- Lien logique (pourquoi cette section aprÃ¨s la prÃ©cÃ©dente)
- Type de transition (cause-effet, chronologique, zoom-in, comparaison, etc.)

Retourne JSON:
{{
  "complexity_analysis": {{
    "topic_complexity": 1-5,
    "specificity": 1-5,
    "format_depth": 1-5,
    "temporal_depth": 1-5,
    "interconnections": 1-5,
    "overall_score": moyenne,
    "target_length": "concis|standard|dÃ©taillÃ©|approfondi",
    "estimated_words": nombre_total,
    "justification": "Pourquoi ce niveau"
  }},

  "sections": ["Titre Section 1", "Titre Section 2", ...],

  "section_targets": {{
    "Titre Section 1": {{
      "words_target": nombre,
      "depth": "light|moderate|deep",
      "objectives": ["objectif 1", "objectif 2"],
      "key_questions": ["question Ã  explorer 1", "question 2"]
    }},
    ...
  }},

  "narrative_flow": [
    {{
      "from_section": "Section 1",
      "to_section": "Section 2",
      "transition_type": "zoom-in|cause-effect|chronological|comparison",
      "rationale": "Pourquoi cet enchaÃ®nement"
    }},
    ...
  ],

  "search_strategy": {{
    "total_sources_needed": nombre (adaptÃ© au score),
    "sources_per_section": nombre,
    "search_depth": "quick|standard|exhaustive"
  }}
}}

EXEMPLES:

Pour "c'est quoi Rust":
{{
  "complexity_analysis": {{"overall_score": 1.2, "target_length": "concis", "estimated_words": 600}},
  "sections": ["DÃ©finition", "Principaux usages"],
  "section_targets": {{
    "DÃ©finition": {{"words_target": 300, "depth": "light", "objectives": ["Expliquer Rust simplement"], "key_questions": ["Qu'est-ce que Rust?"]}},
    "Principaux usages": {{"words_target": 300, "depth": "light", "objectives": ["Lister cas d'usage"], "key_questions": ["Pour quoi utiliser Rust?"]}}
  }},
  "narrative_flow": [{{"from_section": "DÃ©finition", "to_section": "Principaux usages", "transition_type": "zoom-in", "rationale": "AprÃ¨s avoir dÃ©fini, montrer applications concrÃ¨tes"}}],
  "search_strategy": {{"total_sources_needed": 5, "sources_per_section": 2, "search_depth": "quick"}}
}}

Pour "Ã‰cosystÃ¨me Rust 2024, adoption, roadmap":
{{
  "complexity_analysis": {{"overall_score": 4.6, "target_length": "approfondi", "estimated_words": 3500}},
  "sections": ["Vue d'ensemble", "Ã‰cosystÃ¨me technique", "Adoption entreprise", "Cas d'usage", "Roadmap", "DÃ©fis"],
  "section_targets": {{
    "Vue d'ensemble": {{"words_target": 400, "depth": "moderate", "objectives": ["Situer Rust en 2024"], "key_questions": ["Quelle position actuelle?", "Pourquoi pertinent?"]}},
    "Ã‰cosystÃ¨me technique": {{"words_target": 700, "depth": "deep", "objectives": ["Analyser tooling, libs, communautÃ©"], "key_questions": ["Quels outils?", "MaturitÃ©?"]}},
    ...
  }},
  "narrative_flow": [
    {{"from_section": "Vue d'ensemble", "to_section": "Ã‰cosystÃ¨me technique", "transition_type": "zoom-in", "rationale": "AprÃ¨s contexte global, dÃ©tailler aspects techniques"}},
    {{"from_section": "Ã‰cosystÃ¨me technique", "to_section": "Adoption entreprise", "transition_type": "cause-effect", "rationale": "MaturitÃ© technique â†’ adoption business"}},
    ...
  ],
  "search_strategy": {{"total_sources_needed": 15, "sources_per_section": 3, "search_depth": "exhaustive"}}
}}

IMPORTANT: Adapte la structure aux sous-thÃ¨mes dÃ©couverts: {', '.join(topics_found[:5])}
"""

        try:
            response = await self.llm_client.generate(
                [{"role": "user", "content": prompt}],
                max_tokens=3000,
                temperature=0.2
            )

            # Parser JSON
            start_idx = response.find('{')
            if start_idx != -1:
                bracket_depth = 0
                for i, char in enumerate(response[start_idx:], start=start_idx):
                    if char == '{':
                        bracket_depth += 1
                    elif char == '}':
                        bracket_depth -= 1
                        if bracket_depth == 0:
                            plan = json.loads(response[start_idx:i + 1])
                            logger.info(f"     â†’ Plan crÃ©Ã©: {len(plan.get('sections', []))} sections")
                            return plan

        except Exception as e:
            logger.error(f"  âŒ Erreur planification: {e}")

        # Fallback: plan standard
        return {
            "complexity_analysis": {"overall_score": 3.0, "target_length": "standard", "estimated_words": 1200},
            "sections": ["Introduction", "Analyse", "Conclusion"],
            "section_targets": {
                "Introduction": {"words_target": 300, "depth": "moderate", "objectives": ["Introduire le sujet"], "key_questions": []},
                "Analyse": {"words_target": 600, "depth": "moderate", "objectives": ["Analyser le sujet"], "key_questions": []},
                "Conclusion": {"words_target": 300, "depth": "light", "objectives": ["Conclure"], "key_questions": []}
            },
            "narrative_flow": [],
            "search_strategy": {"total_sources_needed": 10, "sources_per_section": 3, "search_depth": "standard"}
        }

    async def _section_research_phase(
        self,
        query: str,
        section_name: str,
        section_config: Dict,
        exploration_data: Dict,
        context: ExecutionContext
    ) -> Dict[str, Any]:
        """
        PHASE 2.1: Recherches ciblÃ©es pour une section spÃ©cifique.

        Retourne sources et donnÃ©es brutes pour cette section.
        """
        logger.info(f"       ðŸ” Recherches pour section '{section_name}'")

        # Construire requÃªte de recherche ciblÃ©e
        key_questions = section_config.get("key_questions", [])
        objectives = section_config.get("objectives", [])

        # RequÃªte enrichie
        search_query = f"{query} {section_name}"
        if key_questions:
            search_query += f" {' '.join(key_questions[:2])}"

        # Nombre de sources selon profondeur
        depth = section_config.get("depth", "moderate")
        max_sources = {"light": 3, "moderate": 5, "deep": 8}.get(depth, 5)

        try:
            search_result = await searxng_client.search(
                query=search_query,
                categories=["general"],
                engines=["google", "duckduckgo"]
            )

            sources = []
            if search_result.get("results"):
                sources = [
                    {
                        "url": r.get("url"),
                        "title": r.get("title", ""),
                        "snippet": r.get("content", "")[:300]
                    }
                    for r in search_result["results"][:max_sources]
                ]

            return {"sources": sources, "query_used": search_query}

        except Exception as e:
            logger.error(f"       âŒ Erreur recherche section: {e}")
            return {"sources": [], "query_used": search_query}

    async def _section_extraction_phase(
        self,
        section_name: str,
        section_data: Dict,
        context: ExecutionContext
    ) -> List[Dict]:
        """
        PHASE 2.2: Extraction du contenu des sources dÃ©couvertes.

        Retourne liste de contenus extraits et nettoyÃ©s.
        """
        logger.info(f"       ðŸ“¥ Extraction sources pour '{section_name}'")

        sources = section_data.get("sources", [])
        urls = [s["url"] for s in sources]

        if not urls:
            return []

        # Utiliser webextractor
        try:
            extractor_manager = ExtractorManager()
            options = ExtractionOptions(
                extract_images=False,
                extract_links=False,
                clean_html=True
            )

            results = await extractor_manager.extract_from_urls(urls, options)

            extracted_data = []
            for result in results:
                if result.get("success") and result.get("content"):
                    extracted_data.append({
                        "source": result.get("url"),
                        "title": result.get("title", ""),
                        "content": result.get("content", ""),
                        "metadata": result.get("metadata", {})
                    })

            logger.info(f"       â†’ {len(extracted_data)}/{len(urls)} sources extraites")
            return extracted_data

        except Exception as e:
            logger.error(f"       âŒ Erreur extraction: {e}")
            return []

    def _cross_reference_data(
        self,
        section_name: str,
        new_data: List[Dict],
        context: ExecutionContext
    ) -> List[Dict]:
        """
        PHASE 2.3: Croisement avec donnÃ©es existantes.

        Enrichit les nouvelles donnÃ©es en identifiant connexions
        avec donnÃ©es dÃ©jÃ  collectÃ©es dans autres sections.
        """
        logger.info(f"       ðŸ”— Croisement donnÃ©es pour '{section_name}'")

        # Pour chaque nouvelle donnÃ©e, chercher mentions dans donnÃ©es existantes
        enriched = []
        for data_point in new_data:
            enriched_point = data_point.copy()
            enriched_point["cross_references"] = []

            # Chercher dans autres sections
            for other_section, section_content in context.final_content['sections'].items():
                if other_section == section_name:
                    continue

                # VÃ©rifier si URL apparaÃ®t dans raw_data
                for existing_data in section_content.get('raw_data', []):
                    if isinstance(existing_data, dict) and existing_data.get('source') == data_point.get('source'):
                        enriched_point["cross_references"].append({
                            "section": other_section,
                            "note": "MÃªme source utilisÃ©e"
                        })

            enriched.append(enriched_point)

        cross_ref_count = sum(1 for e in enriched if e.get("cross_references"))
        if cross_ref_count > 0:
            logger.info(f"       â†’ {cross_ref_count} donnÃ©es avec croisements")

        return enriched

    async def _synthesize_single_section(
        self,
        query: str,
        section_name: str,
        section_config: Dict,
        enriched_data: List[Dict],
        context: ExecutionContext
    ) -> str:
        """
        PHASE 2.4: SynthÃ¨se d'une section unique.

        GÃ©nÃ¨re le contenu rÃ©digÃ© de la section en utilisant:
        - Les donnÃ©es enrichies
        - Les objectifs de la section
        - La profondeur cible
        """
        logger.info(f"       âœï¸  SynthÃ¨se section '{section_name}'")

        depth = section_config.get("depth", "moderate")
        words_target = section_config.get("words_target", 500)
        objectives = section_config.get("objectives", [])
        key_questions = section_config.get("key_questions", [])

        # SÃ©lection sÃ©mantique adaptÃ©e
        selected_data = self._semantic_chunk_selection(
            all_data=enriched_data,
            query=f"{query} {section_name}",
            max_chars=words_target * 10,  # Approximation
            depth=depth
        )

        # Construire le prompt de synthÃ¨se
        depth_instructions = {
            "light": "RÃ©dige 2-3 paragraphes concis allant Ã  l'essentiel.",
            "moderate": "RÃ©dige 4-6 paragraphes Ã©quilibrÃ©s et informatifs.",
            "deep": "RÃ©dige 6-10 paragraphes dÃ©taillÃ©s explorant tous les aspects."
        }.get(depth, "RÃ©dige un contenu Ã©quilibrÃ©.")

        prompt = f"""RÃ©dige la section "{section_name}" pour rÃ©pondre Ã : "{query}"

OBJECTIFS DE CETTE SECTION:
{chr(10).join(f'- {obj}' for obj in objectives)}

QUESTIONS CLÃ‰S Ã€ EXPLORER:
{chr(10).join(f'- {q}' for q in key_questions)}

INSTRUCTIONS:
{depth_instructions}

DONNÃ‰ES DISPONIBLES ({len(selected_data)} sources):
{json.dumps(selected_data[:5], indent=2, ensure_ascii=False)}

RÃˆGLES:
1. Cite TOUTES les sources avec format [SOURCE:url]
2. La longueur sera CONSÃ‰QUENCE NATURELLE de la qualitÃ©, pas un objectif strict
3. Objectif approximatif: ~{words_target} mots
4. Structure: paragraphes cohÃ©rents, pas de listes Ã  puces
5. Ton: informatif, prÃ©cis, fluide

RÃ©dige uniquement le contenu (pas de titre de section, pas de mÃ©tadonnÃ©es).
"""

        try:
            response = await self.llm_client.generate(
                [{"role": "user", "content": prompt}],
                max_tokens=int(words_target * 2.5),
                temperature=0.3
            )

            word_count = len(response.split())
            logger.info(f"       â†’ {word_count} mots gÃ©nÃ©rÃ©s")

            return response.strip()

        except Exception as e:
            logger.error(f"       âŒ Erreur synthÃ¨se: {e}")
            return f"[Erreur lors de la gÃ©nÃ©ration de la section {section_name}]"

    async def _analyze_intersections(
        self,
        query: str,
        plan: Dict,
        context: ExecutionContext
    ) -> Dict[str, Any]:
        """
        PHASE 3.1: Analyse les liens entre sections et identifie amÃ©liorations.

        Retourne:
        - improvements: Liste d'amÃ©liorations de cohÃ©rence Ã  appliquer
        - transitions_needed: Transitions manquantes
        - redundancies: Redondances dÃ©tectÃ©es
        """
        logger.info("  ðŸ” Analyse inter-sections")

        sections_content = []
        for section_name, section_data in context.final_content['sections'].items():
            sections_content.append({
                "title": section_name,
                "content_preview": section_data.get('content', '')[:500],
                "word_count": len(section_data.get('content', '').split()),
                "sources_count": len(section_data.get('raw_data', []))
            })

        prompt = f"""Analyse la cohÃ©rence globale de ce rapport en cours de construction.

REQUÃŠTE INITIALE: "{query}"

SECTIONS GÃ‰NÃ‰RÃ‰ES:
{json.dumps(sections_content, indent=2, ensure_ascii=False)}

ENCHAÃŽNEMENTS PRÃ‰VUS:
{json.dumps(plan.get('narrative_flow', []), indent=2)}

Ta mission: Identifier amÃ©liorations de cohÃ©rence.

Retourne JSON:
{{
  "improvements": [
    {{
      "type": "transition|link|structure",
      "between_sections": ["Section A", "Section B"],
      "issue": "Description du problÃ¨me",
      "suggestion": "Comment amÃ©liorer",
      "priority": "high|medium|low"
    }}
  ],
  "redundancies": [
    {{"sections": ["Section X", "Section Y"], "description": "..."}}
  ],
  "coherence_score": 0-100
}}
"""

        try:
            response = await self.llm_client.generate(
                [{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.2
            )

            start_idx = response.find('{')
            if start_idx != -1:
                bracket_depth = 0
                for i, char in enumerate(response[start_idx:], start=start_idx):
                    if char == '{':
                        bracket_depth += 1
                    elif char == '}':
                        bracket_depth -= 1
                        if bracket_depth == 0:
                            analysis = json.loads(response[start_idx:i + 1])
                            return analysis

        except Exception as e:
            logger.error(f"  âŒ Erreur analyse cohÃ©rence: {e}")

        return {"improvements": [], "redundancies": [], "coherence_score": 75}

    async def _apply_coherence_improvements(
        self,
        coherence_analysis: Dict,
        context: ExecutionContext
    ) -> None:
        """
        PHASE 3.2: Applique les amÃ©liorations de cohÃ©rence identifiÃ©es.

        Modifie les sections in-place pour ajouter transitions, liens, etc.
        """
        improvements = coherence_analysis.get("improvements", [])
        high_priority = [i for i in improvements if i.get("priority") == "high"]

        if not high_priority:
            logger.info("  âœ“ Aucune amÃ©lioration critique nÃ©cessaire")
            return

        logger.info(f"  ðŸ”§ Application de {len(high_priority)} amÃ©liorations")

        for improvement in high_priority[:3]:  # Limiter Ã  3 max
            imp_type = improvement.get("type")
            sections_involved = improvement.get("between_sections", [])
            suggestion = improvement.get("suggestion", "")

            if imp_type == "transition" and len(sections_involved) == 2:
                # Ajouter transition entre deux sections
                section_a, section_b = sections_involved
                if section_a in context.final_content['sections'] and section_b in context.final_content['sections']:
                    # Ajouter phrase de transition Ã  la fin de section A
                    current_content = context.final_content['sections'][section_a]['content']
                    transition_text = f"\n\n{suggestion}"
                    context.final_content['sections'][section_a]['content'] = current_content + transition_text
                    logger.info(f"     â†’ Transition ajoutÃ©e: {section_a} â†’ {section_b}")

    async def _final_assembly(
        self,
        query: str,
        plan: Dict,
        context: ExecutionContext
    ) -> Dict[str, Any]:
        """
        PHASE 4.1: Assemblage final du rapport complet.

        Combine toutes les sections en rapport structurÃ©.
        """
        logger.info("  ðŸ“¦ Assemblage final")

        sections_list = []
        total_words = 0

        for section_name in plan.get("sections", []):
            if section_name in context.final_content['sections']:
                section_data = context.final_content['sections'][section_name]
                content = section_data.get('content', '')
                word_count = len(content.split())
                total_words += word_count

                sections_list.append({
                    "title": section_name,
                    "content": content,
                    "data": section_data.get('raw_data', []),
                    "metadata": {
                        "word_count": word_count,
                        "sources_count": len(section_data.get('raw_data', []))
                    }
                })

        # GÃ©nÃ©rer titre et summary
        complexity = plan.get("complexity_analysis", {})
        title = f"Analyse: {query.title()}"

        # GÃ©nÃ©rer summary (rÃ©sumÃ© du premier paragraphe de la premiÃ¨re section)
        summary = ""
        if sections_list:
            first_content = sections_list[0].get("content", "")
            # Prendre les 2 premiÃ¨res phrases
            sentences = first_content.split('. ')[:2]
            summary = '. '.join(sentences) + '.' if sentences else "Rapport de recherche approfondie."

        final_report = {
            "type": "report",
            "title": title,
            "summary": summary,
            "sections": sections_list,
            "metadata": {
                "total_word_count": total_words,
                "sections_count": len(sections_list),
                "complexity_score": complexity.get("overall_score", 3.0),
                "target_length": complexity.get("target_length", "standard")
            }
        }

        # Conversion sources â†’ bibliographie
        final_report = self._convert_sources_to_bibliography(final_report)

        logger.info(f"  âœ“ Rapport assemblÃ©: {total_words} mots, {len(sections_list)} sections")

        return final_report

    def _add_complete_traces(
        self,
        report: Dict[str, Any],
        context: ExecutionContext,
        exploration_data: Dict,
        plan: Dict
    ) -> Dict[str, Any]:
        """
        PHASE 4.3: Ajoute traces complÃ¨tes du processus au rapport.

        Permet de suivre toute la recherche de A Ã  Z.
        """
        logger.info("  ðŸ“‹ Ajout traces complÃ¨tes")

        report["research_traces"] = {
            "exploration_phase": {
                "sources_discovered": len(exploration_data.get("sources", [])),
                "field_assessment": exploration_data.get("field_assessment"),
                "topics_identified": exploration_data.get("topics_found", [])
            },
            "planning_phase": {
                "complexity_analysis": plan.get("complexity_analysis", {}),
                "sections_planned": plan.get("sections", []),
                "narrative_flow": plan.get("narrative_flow", []),
                "search_strategy": plan.get("search_strategy", {})
            },
            "construction_phase": {
                "sections_built": list(context.final_content['sections'].keys()),
                "total_sources_collected": sum(
                    len(s.get('raw_data', []))
                    for s in context.final_content['sections'].values()
                ),
                "steps_executed": len(context.steps)
            },
            "coherence_phase": {
                "improvements_applied": "See coherence analysis"
            }
        }

        logger.info("  âœ“ Traces complÃ¨tes ajoutÃ©es")

        return report
